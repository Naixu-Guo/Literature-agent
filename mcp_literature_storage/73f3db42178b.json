{
  "id": "73f3db42178b",
  "source": "/var/folders/05/6f0vjthd471ff1qt42rb7nlw0000gn/T/tmpsquolp48.pdf",
  "metadata": {
    "title": "https://arxiv.org/pdf/2402.16714.pdf",
    "source_type": "pdf",
    "path": "/var/folders/05/6f0vjthd471ff1qt42rb7nlw0000gn/T/tmpsquolp48.pdf",
    "pages": 31,
    "file_size": 1529526
  },
  "chunks": [
    "Quantum linear algebra is all you need for Transformer architectures\nNaixu Guo, 1, \u2217 Zhan Yu,1, \u2020 Matthew Choi, 2, 3 Aman Agrawal,4 Kouhei\nNakaji,5, 6, 7 Al\u00b4 an Aspuru-Guzik,2, 3, 5, 8, 9, 10and Patrick Rebentrost 1, 11, \u2021\n1Centre for Quantum Technologies, National University of Singapore, 117543, Singapore\n2Department of Computer Science, University of Toronto, Toronto, Ontario M5S 2E4, Canada\n3Vector Institute for Artificial Intelligence, Toronto, Ontario M5S 1M1, Canada\n4Department of Mathematics, National University of Singapore, 119076, Singapore\n5Department of Chemistry, University of Toronto, Toronto, Ontario M5G 1Z8, Canada\n6Research Center for Emerging Computing Technologies, National Institute of Advanced\nIndustrial Science and Technology (AIST), 1-1-1 Umezono, Tsukuba, Ibaraki 305-8568, Japan\n7Quantum Computing Center, Keio University, 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Kanagawa, 223-8522, Japan",
    "Industrial Science and Technology (AIST), 1-1-1 Umezono, Tsukuba, Ibaraki 305-8568, Japan\n7Quantum Computing Center, Keio University, 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Kanagawa, 223-8522, Japan\n8Department of Materials Science & Engineering, University of Toronto, Toronto, Ontario M5S 3E4, Canada\n9Department of Chemical Engineering & Applied Chemistry,\nUniversity of Toronto, Toronto, Ontario M5S 3E5, Canada\n10Lebovic Fellow, Canadian Institute for Advanced Research, Toronto, Ontario M5G 1Z8, Canada\n11School of Computing, National University of Singapore, 117417, Singapore\n(Dated: June 3, 2024)\nGenerative machine learning methods such as large-language models are revolutionizing the cre-\nation of text and images. While these models are powerful they also harness a large amount of\ncomputational resources. The transformer is a key component in large language models that aims\nto generate a suitable completion of a given partial sequence. In this work, we investigate trans-",
    "computational resources. The transformer is a key component in large language models that aims\nto generate a suitable completion of a given partial sequence. In this work, we investigate trans-\nformer architectures under the lens of fault-tolerant quantum computing. The input model is one\nwhere trained weight matrices are given as block encodings and we construct the query, key, and\nvalue matrices for the transformer. We show how to prepare a block encoding of the self-attention\nmatrix, with a new subroutine for the row-wise application of the softmax function. In addition,\nwe combine quantum subroutines to construct important building blocks in the transformer, the\nresidual connection and layer normalization, and the feed-forward neural network. Our subrou-\ntines prepare an amplitude encoding of the transformer output, which can be measured to obtain\na prediction. Based on common open-source large-language models, we provide insights into the",
    "tines prepare an amplitude encoding of the transformer output, which can be measured to obtain\na prediction. Based on common open-source large-language models, we provide insights into the\nbehavior of important parameters determining the run time of the quantum algorithm. We discuss\nthe potential and challenges for obtaining a quantum advantage.\nCONTENTS\nI. Introduction 2\nII. Preliminary 3\nA. Notation 3\nB. Brief description about transformer 4\nC. Quantum procedures 6\nIII. Problem formulations 8\nIV. Main results 9\nA. Element-wise function of block-encoded matrices 9\nB. Conversion between state preparation encoding and matrix block encoding 11\nC. Quantum self-attention 12\n\u2217 naixug@u.nus.edu\n\u2020 yu.zhan@u.nus.edu\n\u2021 cqtfpr@nus.edu.sg\narXiv:2402.16714v2  [quant-ph]  31 May 2024D. Quantum residual connection and layer normalization 14\nE. Quantum feedforward network 16\nF. Quantum single-layer transformer 17\nV. Numerical studies of quantum-relevant properties of real-world LLMs 18",
    "E. Quantum feedforward network 16\nF. Quantum single-layer transformer 17\nV. Numerical studies of quantum-relevant properties of real-world LLMs 18\nVI. Extensions 20\nVII. Discussion 21\nReferences 22\nA. Construction of block encoding unitaries 25\nB. Robust nonlinear amplitude transformation 26\nC. Matrix maximum entry norm 26\nD. Normalized error bound 26\nE. Polynomial approximation of exponential function 29\nF. General case of quantum residual connection 29\nG. Quantum single-layer transformer 31\nI. INTRODUCTION\nLarge language models (LLMs) such as GPT4 recently arrived in the public consciousness and continue to\nmake headlines [1, 2]. The transformer architecture has emerged as the dominant model architecture for these\nLLMs [3]. One of the primary tasks addressed by the transformer is to generate an output sequence based on\nthe input sequence, such as a sentence of English words. It was designed to \u201clearn what to pay attention to\u201d,",
    "the input sequence, such as a sentence of English words. It was designed to \u201clearn what to pay attention to\u201d,\nwith the self-attention block capturing correlations among different parts of the sequence via inner products\n[3, 4]. The architecture consists of two main components: the encoder and the decoder. These components\nare largely similar, both constructed using self-attention blocks and feed-forward neural networks. In recent\nyears, the decoder-only structure has become prevalent, corresponding to an auto-regressive model that can\nbe used for next token prediction [5\u20138]. Despite its many advantages, the transformer architecture has several\ndrawbacks, particularly the computational resources required for inference. Addressing this issue is crucial\nfrom both scientific and societal perspectives.\nQuantum computing has been investigated for linear algebra-based tasks for the last decades, demon-",
    "from both scientific and societal perspectives.\nQuantum computing has been investigated for linear algebra-based tasks for the last decades, demon-\nstrating the potential for quantum advantages in solving linear systems and performing other matrix linear\nalgebra operations [9, 10]. Quantum singular value transformation has become a unified framework for\nquantum algorithms based on block encodings and polynomial transformations on matrices [11, 12]. It has\nrecently been generalized to non-normal matrix cases [13]. Significant progress in hardware has improved\nboth the quantity and quality of quantum bits [14, 15], with recent experiments achieving systems with 10s\nof logical qubits [16]. While the hardware is still in its early stages, it is valuable to discuss if quantum\ncomputers could, in principle, provide any advantage for large-language models, especially for saving the\ncomputational cost of inference. Therefore, it is a worthwhile goal to explore the application of advanced",
    "computational cost of inference. Therefore, it is a worthwhile goal to explore the application of advanced\nquantum algorithms in constructing state-of-the-art machine learning algorithms.\nA few problems emerge when considering the application of quantum computing to LLMs. First, LLMs\nare based on large data sets, like terabytes of input data. Quantum computers so far are not good at\nbig classical data applications, as the proposals of quantum Random Access Memory (qRAM) are hard\nto realize in practice [17, 18]. Second, modern LLMs contain billions of training parameters. Current\nquantum computers are of the size at most few thousands of qubits and even if every qubit carries several\n2training parameters, overall the number of training parameters is vanishingly small compared to the advanced\nclassical models. Third, the no-cloning principle for quantum states holds in general. In classical information",
    "classical models. Third, the no-cloning principle for quantum states holds in general. In classical information\nprocessing, it is natural to save computed data to memory for later access. With quantum computing, such\na step should in general be avoided as it incurs the cost of full state tomography, which often destroys\npossibilities of quantum advantage.\nIn this work, we show progress towards an end-to-end transformer architecture, which includes all the\nkey building blocks and a discussion of their quantum complexity. We work in the fault-tolerant model of\nquantum computation and use the framework of block encodings and quantum singular-value transformations\n[11, 12, 19]. The modularity of this framework makes it a natural candidate for transformer architectures.\nOur first simplification is that we assume the transformer is already trained, i.e., we are given the pretrained\nweight matrices for query, key, and value parts as quantum circuits. The second simplification is that we",
    "weight matrices for query, key, and value parts as quantum circuits. The second simplification is that we\nfocus on the inference process, i.e., the prediction of a single next token. We develop quantum subroutines for\nself-attention, residual connection and layer normalization, and feed-forward neural networks, and combine\nthem into a single-layer transformer architecture. One of our main technical contributions is a subroutine for\nimplementing the elementwise functions of block encodings, which we use to perform the softmax function\nin the self-attention block. We also show how to implement the Hadamard product of block encodings, a\nsubroutine of independent interest. Our subroutines are efficient in their use of qubits and circuit depth,\nwhich allows for the potential for a variety of quantum improvement not limited to quadratic speedups\nunder a certain regime. We further investigate and verify input assumptions for the regime by performing",
    "under a certain regime. We further investigate and verify input assumptions for the regime by performing\nnumerical experiments on several open-source large language models.\nThe output of our algorithm is a quantum circuit which is a block encoding of the transformer architecture\nand is able to prepare the output of a single-layer transformer as an amplitude-encoded quantum state.\nThis block encoding can then be used for subsequent layers of a neural network architecture. In addition, a\nsubsequent transformation of the amplitude-encoded state and measuring in the computational basis outputs\nthe index of the next predicted token according to the probabilities modeled by the transformer architecture.\nOur main results can be summarized in the following informal theorem.\nTheorem 1 (Quantum single-layer Transformer, informal). For a transformer, see Fig. 1, with embedding\ndimension d and an input sequence S of length N = 2n, assume block-encoded inputs with encoding nor-",
    "dimension d and an input sequence S of length N = 2n, assume block-encoded inputs with encoding nor-\nmalization factors at most \u03b1 and certain normalization factors being constants. If all encoding errors are\nat most \u03f5block = O(\u03f58/d4N), then for the index j \u2208 [N], one can construct an \u03f5-accurate state preparation\nquantum circuit for the quantum state proportional to\ndX\nk=1\nTransformer(S, j)k|k\u27e9, (1)\nby using the input block encodings for O(dn2\u03b12 log2(1/\u03f5block)) times.\nThe paper is organized as follows. The preliminary is provided in Section II, where we introduce the\nnotations, descriptions about the transformer, and existing quantum subroutines. In Section III, we formulate\nthe classical transformer building blocks into quantum problems. Following this, we show our main results\nin Section IV. In Section VII, we discuss the input assumptions, generalization to multi-layer architectures,\npotential quantum advantages, and future research directions.\nII. PRELIMINARY\nA. Notation",
    "in Section IV. In Section VII, we discuss the input assumptions, generalization to multi-layer architectures,\npotential quantum advantages, and future research directions.\nII. PRELIMINARY\nA. Notation\nWe use the Dirac notation |\u03c8\u27e9 to represent a vector with \u2225\u03c8\u22252 = 1 (pure quantum state). Denote by N\nthe natural numbers {1, 2, \u00b7\u00b7\u00b7} . For N \u2208 N , we use the notation [ N] to represent the set {1, . . . , N}. For\nan n-qubit state |0\u27e9\u2297n, we write |0n\u27e9 for simplicity. When there is no ambiguity, we may further ignore\nthe superscript n of |0n\u27e9. For a matrix or an operator A, we use Ajk := \u27e8j|A|k\u27e9 to represent its ( j, k)-th\nelement, where {|k\u27e9} are the standard basis. We use Aj\u22c6 to represent its j-th row and A\u22c6k to represent\n3(Masked) Self Attention\nBlock Encoding\nLayer Norm\nFeed-Forward Network\nLayer Norm\nInput sequence\nWeight matrices\nQuantum residual connection with layer normalizationfor the \ud835\udc57-thtoken\nQuantum self-attention matrix",
    "Block Encoding\nLayer Norm\nFeed-Forward Network\nLayer Norm\nInput sequence\nWeight matrices\nQuantum residual connection with layer normalizationfor the \ud835\udc57-thtoken\nQuantum self-attention matrix\n<latexit sha1_base64=\"fFKNJu8fRJYkhs8SafCmd5s3XAU=\">AAACDnicbVC7TsMwFHXKo6XlUWBksagqdUBVwlAYK1gYC6IPqQmR4zitVccJtlOpivoFMPArLAwgxMrMxm8wM+A+Bmg50pWOzrlX997jxYxKZZqfRmZldW09m9vIFza3tneKu3stGSUCkyaOWCQ6HpKEUU6aiipGOrEgKPQYaXuD84nfHhIhacSv1SgmToh6nAYUI6Ult1huu7dHbXega2hTDu0Qqb7npVfjm9S3FQ2JhP7YLZbMqjkFXCbWnJTqha+zbOH+u+EWP2w/wklIuMIMSdm1zFg5KRKKYkbGeTuRJEZ4gHqkqylHeo+TTt8Zw7JWfBhEQhdXcKr+nkhRKOUo9HTn5Fq56E3E/7xuooJTJ6U8ThTheLYoSBhUEZxkA30qCFZspAnCgupbIe4jgbDSCeZ1CNbiy8ukdVy1atXapU6jAmbIgQNwCCrAAiegDi5AAzQBBnfgETyDF+PBeDJejbdZa8aYz+yDPzDefwAVYp8S</latexit>\nW q ,W k ,W v 2 Rd\u21e5d",
    "W q ,W k ,W v 2 Rd\u21e5d\n<latexit sha1_base64=\"8CBtkLGuEFB1brlPUmdPFf2FYsY=\">AAACBHicbVC7TsMwFHXKq5RXgbGLRYXUqUoYChNUYmFC5dGH1ITKcZzWquNEtoNURR1Y+BUWBhBi5QfY2Nj6BSz8AE7bAVqOdKWjc+7Vvfe4EaNSmeankVlYXFpeya7m1tY3Nrfy2zsNGcYCkzoOWShaLpKEUU7qiipGWpEgKHAZabr909Rv3hIhaciv1SAiToC6nPoUI6WlTr5wZVMO7QCpnusml8Ob5NxWNCASesNOvmiWzTHgPLGmpHgy+hp9vx83a538h+2FOA4IV5ghKduWGSknQUJRzMgwZ8eSRAj3UZe0NeVI73GS8RNDuK8VD/qh0MUVHKu/JxIUSDkIXN2ZXitnvVT8z2vHyj9yEsqjWBGOJ4v8mEEVwjQR6FFBsGIDTRAWVN8KcQ8JhJXOLadDsGZfnieNg7JVKVcuzGK1BCbIggLYAyVggUNQBWegBuoAgzvwAJ7As3FvPBovxuukNWNMZ3bBHxhvP7d0nUY=</latexit>\nS 2 RN \u21e5d\nQuantum feed-forward network with anactivation function \ud835\udf0eand aninput vector \ud835\udf13\nBlock encoding of the input matrices",
    "S 2 RN \u21e5d\nQuantum feed-forward network with anactivation function \ud835\udf0eand aninput vector \ud835\udf13\nBlock encoding of the input matrices\n<latexit sha1_base64=\"NMDQE8ww6vmGdJj1DX5d3rFZ6CA=\">AAACVHicbVDLSsNAFJ2mvp9Vl24GRXQhJelCXYpu3AgqVoWmhslkooOTmTBzI5YQ3PgTLt35BX6GSx/fogsnrYi1XrhwOPdczr0nTAU34LrvFac6NDwyOjY+MTk1PTNbm5s/MSrTlDWpEkqfhcQwwSVrAgfBzlLNSBIKdhpe7Zbz02umDVfyGDopayfkQvKYUwKWCmo7+4GHfS6xnxC4DMP8qDjPo1XsA0+YwVGxjveDxqDiR7BaBLVlt+52Cw8C7xssb1dXbp/u7x4OgtqHHymaJUwCFcSYluem0M6JBk4FKyb8zLCU0CtywVoWSmKN2nn31wKvWCbCsdK2JeAu+3sjJ4kxnSS0yvJe83dWkv/NWhnEW+2cyzQDJmnPKM4EBoXL4HDENaMgOhYQqrm9FdNLogkFG2+fy01s2b438tIOlBKmTMv7m80gOGnUvY36xqGNbQ31agwtoiW0hjy0ibbRHjpATUTRI3pGr+it8lL5dKrOcE/qVL53FlBfOTNfT5m4bg==</latexit>\nM 1 2 Rd0 \u21e5d ,M 2 2 Rd\u21e5d0",
    "M 1 2 Rd0 \u21e5d ,M 2 2 Rd\u21e5d0\n<latexit sha1_base64=\"4du6gqwST5Q0LAsMEYC/GPRvMsU=\">AAACWHicbZDPThRBEMZrR4VdFFjwyKUjMVkuywwH9EJCYqJcTDBxgWRnnfT09Ayd6T+T7hrCZtwn8RV8Gm+oR98CD/bsYuKClXTy5VdV+bq+tJLCYRjedIJHj5+srHZ7a0+frW9s9re2z5ypLeMjZqSxFyl1XArNRyhQ8ovKcqpSyc/T8k3bP7/i1gmjP+K04hNFCy1ywSh6lPTfxq5WSVMeRbNPGYlTUcjB++SAxCwzSGInCkWJJ9FfUjmx147ZvaQkn8vYUl1InvR3w2E4L/JQRHdi93g//f3u1xdymvRv48ywWnGNTFLnxlFY4aShFgWTfLYW145XlJW04GMvNVXcTZr5vTPy0pOM5Mb6p5HM6b8bDVXOTVXqJxXFS3e/18L/9cY15q8njdBVjVyzhVFeS4KGtOGRTFjOUE69oMwK/1fCLqmlDH3ESy7XuadLZzStHRoj3cynFd3P5qE4OxhGh8PDDz62ASyqCzvwAgYQwSs4hhM4hREw+Arf4Af87HwPIFgNeovRoHO38xyWKtj+AwMeuVU=</latexit>\ndX\nk =1\n\u0000\nM 2 \u00b7 \u0000 (M 1 \u00b7  )\n\u0000\nk |ki",
    "dX\nk =1\n\u0000\nM 2 \u00b7 \u0000 (M 1 \u00b7  )\n\u0000\nk |ki\n<latexit sha1_base64=\"v2fGDNa3BOsOpWjhHpBSFWSOLbQ=\">AAACQ3icbVC7bhNBFJ0NARyHgIGSZpQIyWmc3RSB0pKLRKKxhe1E8jrW3fFsPMo81jN3o5jVfhAfQkVBh8gnIDpEi8Ss7SIPjjTS0Tn3NSfJpHAYhjfBxqPNx0+e1rbq2892nr9ovHw1dCa3jA+YkcaeJeC4FJoPUKDkZ5nloBLJT5PLTuWfXnHrhNF9XGR8rOBCi1QwQC9NGp3j8yJWgDOrCmdSLEsaV1M1n8/pbUPBddnsfTjvH8QgsxlMwn1fOTVIh5PGXtgKl6APSbQme+3ap+9fP3a3upPGz3hqWK64RibBuVEUZjguwKJgkpf1OHc8A3YJF3zkqQbF3bhYfrakb70ypamx/mmkS/V2RwHKuYVKfGV1vrvvVeL/vFGO6ftxIXSWI9dstSjNJUVDq+ToVFjOUC48AWaFv5WyGVhg6PO9s6WajcZIV9Z9NtH9JB6S4WErOmod9XxITbJCjbwhu6RJIvKOtMkJ6ZIBYeQz+UZ+kJvgS/Ar+B38WZVuBOue1+QOgr//AD/HtY8=</latexit>\nGsoft := softmax( QK T /\u21b50 ) \u00b7 V",
    "Gsoft := softmax( QK T /\u21b50 ) \u00b7 V\n<latexit sha1_base64=\"xJizW/lvUxlTt70q/Zo7Q+r/hGk=\">AAAC3nicnZJBaxNBFMdnV2vbtWqqRy+DwVKKrLu2tB4LXgQvLZq0kA3x7eQlGTozu8y8bRuWHLx4UEqv/Vze/CDenaRBtBEKfTDMn/97P+bNm8lLJR0lyc8gvHd/6cHyymr0cO3R4yeN9adtV1RWYEsUqrDHOThU0mCLJCk8Li2CzhUe5SfvpvmjU7ROFuYTjUvsahgaOZACyFu9xq/MosEzUWgNpl9nYC2MHVkkMZrUabw9ibIch9LUuQay8nwSfXydgSpHwDf4VpZFW9M9ytDTf0peLUCHd4E+3AVq3w7FvUYziZNZ8EWRzkWTzeOg1/iR9QtRaTQkFDjXSZOSujVYkkKhn1HlsARxAkPseGlAo+vWs+eZ8Jfe6fNBYf0yxGfu30QN2rmxzn2l73Dkbuam5v9ynYoGb7u1NGVFaMT1QYNKcSr49K15X1oUpMZegLDS98rFCCwI8j8i8kNIb155UbTfxOluvHu409zfnI9jhT1nL9gmS9ke22fv2QFrMRF0gi/Bt+B7+Dn8Gl6El9elYTBnnrF/Irz6DaNh3uA=</latexit>\n\"\nS/ \u21b5 \u21e4\n\u21e4\u21e4\n#\n,\n\"\nQ/ \u21b5 \u21e4\n\u21e4\u21e4\n#\n,\n\"\nK/ \u21b5 \u21e4\n\u21e4\u21e4\n#\n,\n\"\nV/ \u21b5 \u21e4\n\u21e4\u21e4\n#\n.",
    "\"\nS/ \u21b5 \u21e4\n\u21e4\u21e4\n#\n,\n\"\nQ/ \u21b5 \u21e4\n\u21e4\u21e4\n#\n,\n\"\nK/ \u21b5 \u21e4\n\u21e4\u21e4\n#\n,\n\"\nV/ \u21b5 \u21e4\n\u21e4\u21e4\n#\n.\n<latexit sha1_base64=\"C54unuoyOhuDss2m+vMy2PsmPT4=\">AAACJHicbVA9SyRBEO3xvFPXu3PV0KRRDvbg2JsxUEEE4QINRBRvVdhZh57emrWdnp6hu0ZcxvklRib+FRMDPzAwMfJnXHC9uwZ+PSh4/V4VXfXCTAqDrvvgDH0a/vxlZHSsMv712/eJ6uTUrklzzaHBU5nq/ZAZkEJBAwVK2M80sCSUsBfGf3r+3jFoI1L1F7sZtBLWUSISnKGVguqyb/IkKOIVrzxoUx/hBIuNzbK2dlAMHiaNsCyDo190Jzj6GcT0lMa+ZqojIajOuXW3D/qeeM9kbvV3+G/t6YxuBdVbv53yPAGFXDJjmp6bYatgGgWXUFb83EDGeMw60LRUsQRMq+gfWdIfVmnTKNW2FNK++nKiYIkx3SS0nQnDQ/PW64kfec0co6VWIVSWIyg++CjKJcWU9hKjbaGBo+xawrgWdlfKD5lmHG2uFRuC9/bk92R3vu4t1Be2bRo1MsAomSGzpEY8skhWyTrZIg3CyTm5JNfkxrlwrpw7537QOuQ8z0yTV3Ae/wM5TajJ</latexit>\ndX\nk =1\nLN( G soft\nj ,S j )k |k i\nFIG. 1. Overview of the quantum transformer architecture.Here, we visualize a single-layer transformer\narchitecture and highlight the parts relevant to this work. We construct and discuss the corresponding quantum",
    "architecture and highlight the parts relevant to this work. We construct and discuss the corresponding quantum\nsubroutines and combine them to our final theorem on obtaining a sample from the first layer output. The inputs\nare matrices for the input sequence and pretrained weights, from which the relevant matrices for the transformer are\nconstructed (query Q, key K, and value V ). These inputs are given as block encodings, where the block encoding of\na matrix is a unitary matrix that contains the desired normalized matrix in a diagonal block. The building blocks\nand symbols are further explained in Section II B.\nits k-th column. The spectral norm, i.e., the largest singular value, is denoted by \u2225A\u2225. We write \u2225A\u2225F\nto represent the Frobenius norm. For a normal matrix A := P\nk \u03bbk(A)|\u03c8k\u27e9\u27e8\u03c8k| and a function f, we write\nf(A) := P\nk f(\u03bbk(A))|\u03c8k\u27e9\u27e8\u03c8k| to represent the eigenvalue transformation of A with f. For a matrix A and",
    "k \u03bbk(A)|\u03c8k\u27e9\u27e8\u03c8k| and a function f, we write\nf(A) := P\nk f(\u03bbk(A))|\u03c8k\u27e9\u27e8\u03c8k| to represent the eigenvalue transformation of A with f. For a matrix A and\na function f, we use f \u25e6 (A) to represent the element-wise application of the function to the matrix, i.e.,\n(f \u25e6 (A))jk = f(Ajk).\nB. Brief description about transformer\nThe transformer is a key component of pretrained foundation models. It has many applications and one of\nthe main ones is the next token prediction, which has achieved great success in natural language processing.\nGiven a part of a sequence, the transformer aims to predict the next object of the sequence. The transformer\nis constructed by three main building blocks: self-attention, residual connection with layer normalization,\nand feed-forward networks (FFN). These building blocks will be described in this section. The original paper\n[3] contains both the encoder and decoder parts. Later many practically significant models only use one",
    "[3] contains both the encoder and decoder parts. Later many practically significant models only use one\npart, especially the decoder-only structure, which is shown in Fig. 1.\nA key aspect of large-language models is tokenization. The token is the basic unit of the transformer\nprocess. Concepts like words, codes, and images can be converted to tokens with the so-called tokenization\nmethod [20\u201322]. For the transformer, tokens are further mapped to real vectors via embedding [3]. Let dtoken\nbe the number of tokens in the dictionary of the machine learning model and dmodel be the dimension of the\nvectors of the embedding. Let W := {\u03c9j \u2208 Rdmodel : \u03c9j is the embedding of token j \u2208 [dtoken]} be the set of\n4the embedding vectors of all tokens. For simplicity, when we mention tokens in this paper, we directly mean\ntheir vector representations. An N-length sentence is a sequence of vectors {Sj}N\nj=1, where Sj \u2208 W. Due to",
    "their vector representations. An N-length sentence is a sequence of vectors {Sj}N\nj=1, where Sj \u2208 W. Due to\nthe vector embeddings of the tokens, a sentence can also be understood as a real matrix S \u2208 RN\u00d7dmodel .\nSelf-attention \u2014 The correlations of the original concepts, such as words in natural languages, imply\ncorrelations of the corresponding tokens in the set of tokens. Self-attention is the building block to encode\nsuch correlation information among tokens (vectors) into a new vector, which is the input vector for the\nnext block. The correlation is computed via estimating inner products. The block is also called the \u201cscaled\ndot-product attention\u201d.\nThere are three real parameterized (weight) matrices Wq, Wk \u2208 Rdmodel\u00d7dk and Wv \u2208 Rdmodel\u00d7dv arising in\nthe self-attention block. In practical cases, dmodel = dk = dv is widely used, e.g., in the original paper [3].\nIn our discussion, we will keep this condition and write d := dmodel for simplicity. Given the sentence S, the",
    "In our discussion, we will keep this condition and write d := dmodel for simplicity. Given the sentence S, the\nconvention is to call Q := SWq, K := SWk, and V := SWv the query, key, and value matrices respectively.\nThe attention block computes the matrix Gsoft \u2208 RN\u00d7d such that\nAttention(Q, K, V) = Attention(S) = softmax(QKT /\u03b10)V =: Gsoft, (2)\nwhere \u03b10 > 0 is a scaling factor, and softmax( z)j := ezj /(P\nk\u2208[N] ezk ) for z \u2208 R N and j \u2208 [N].\nIn the attention block, the softmax is implemented for each row of the matrix QKT /\u03b10. Formally, for a\nmatrix M \u2208 R N\u00d7N , it is defined as a row-wise application of the softmax function, i.e., softmax( M)ij :=\neMij /(P\nk\u2208[N] eMik ) for i, j\u2208 [N]. The factor \u03b10 controls that the exponentiated values are not too large.\nThe value\u03b10 =\n\u221a\nd has been discovered to be a good choice in practice. To see this, assume that each row ofQ\nand K has zero mean and unit standard deviation. Then for each element of (QKT )jk = Pd\nm=1 QjmKkm, the",
    "\u221a\nd has been discovered to be a good choice in practice. To see this, assume that each row ofQ\nand K has zero mean and unit standard deviation. Then for each element of (QKT )jk = Pd\nm=1 QjmKkm, the\nstandard deviation will be bounded by\n\u221a\nd. The coefficient rescales the standard deviation to 1. Depending\non the architecture and embeddings other scaling factors may also be employed [23, 24]. Inspired from the\nblock-encoding discussion in this work, there is a natural choice for this scaling as we discuss in Section IV C.\nFor j \u2208 [N], if the current query token is the j-th token Sj, the corresponding output vector is the j-th\nrow of the self-attention matrix in Eq. (2), denoted by Gsoft\nj . More explicitly, the output vector of the\nself-attention layer for the j-th token is\nGsoft\nj =\ndX\nk=1\nGsoft\njk \u02c6ek \u2261 (Gsoft)T \u02c6ej, (3)\nwhere {\u02c6ej}N\nj=1 is the standard basis. For the decoder-only structure which achieves the best practical",
    "self-attention layer for the j-th token is\nGsoft\nj =\ndX\nk=1\nGsoft\njk \u02c6ek \u2261 (Gsoft)T \u02c6ej, (3)\nwhere {\u02c6ej}N\nj=1 is the standard basis. For the decoder-only structure which achieves the best practical\nperformance, the so-called masked self-attention is used, which has the effect to mask or hide the tokens\nafter the current query token. This is achieved by adding a masked matrix QKT \u2192 QKT + M, where\nMjk =\n(\n0 k \u2264 j,\n\u2212\u221e k > j. (4)\nSince exp(\u2212\u221e) = 0, tokens with index larger than j receive no attention. A further generalization called the\nmulti-head self-attention is based on computing several smaller attention matrices and concatenating them\ntogether. The h-head self attention can be achieved with linear transformations WQ\ni , WK\ni , WV\ni \u2208 Rd\u00d7\u2308 d\nh \u2309,\nand WO \u2208 Rd\u00d7d for i \u2208 [h]:\nMultihead(Q, K, V) = [head1, . . . ,headh]WO \u2208 RN\u00d7d,\nwhere headi = Attention(QWQ\ni , KWK\ni , V WV\ni ) \u2208 RN\u00d7\u2308 d\nh \u2309.",
    "i , WK\ni , WV\ni \u2208 Rd\u00d7\u2308 d\nh \u2309,\nand WO \u2208 Rd\u00d7d for i \u2208 [h]:\nMultihead(Q, K, V) = [head1, . . . ,headh]WO \u2208 RN\u00d7d,\nwhere headi = Attention(QWQ\ni , KWK\ni , V WV\ni ) \u2208 RN\u00d7\u2308 d\nh \u2309.\nResidual connection \u2014 For a computation block like the self-attention, a residual connection with subse-\nquent layer normalization is employed. This layer provides the ability to skip the computation block. We\ntake the self-attention as an example. Note that if we focus on the j-th token, Sj can be understood as the\ninput and Gsoft\nj \u2261 Attention(S, j) is the output vector of the self-attention block. The residual connection\n5gives the output vector Gsoft\nj + Sj1. The next step is the layer normalization, which is to standardize the\nvector. Let \u00afsj := 1\nd\nPd\nk=1(Gsoft\njk + Sjk) \u00b7\u02c6I, where \u02c6I = (1, . . . ,1) and \u03c2 :=\nq\n1\nd\nPd\nk=1((Gsoft\nj + Sj \u2212 \u00afsj)k)2. The\ncomplete residual connection with the normalization layer can be expressed as\nLN\u03b3,\u03b2 (Gsoft\nj , Sj) = \u03b3 Gsoft\nj + Sj \u2212 \u00afsj\n\u03c2 + \u03b2, (5)",
    "q\n1\nd\nPd\nk=1((Gsoft\nj + Sj \u2212 \u00afsj)k)2. The\ncomplete residual connection with the normalization layer can be expressed as\nLN\u03b3,\u03b2 (Gsoft\nj , Sj) = \u03b3 Gsoft\nj + Sj \u2212 \u00afsj\n\u03c2 + \u03b2, (5)\nwhere \u03b3 is the scaling factor and\u03b2 is the biased vector. For simplicity, we may not write these factors explicitly\nwhen there is no confusion. We write LN\u03b3,\u03b2 (Gsoft\nj , Sj)k to represent thek-th element, i.e., (LN\u03b3,\u03b2 (Gsoft\nj , Sj))k.\nThe role of this part is to improve the trainability, which has been found essential for training deep neural\nnetworks in practice [25, 26].\nFeed-forward network \u2014 Finally, a two-layer fully-connected feed-forward network is implemented, i.e.,\nFFN(LN(zj, Sj)) = \u03c3(LN(Gsoft\nj , Sj)M1 + b1)M2 + b2, (6)\nwhere \u03c3 is an activation function, such as tanh( x) and ReLU( x) = max(0, x). Another activation function\nthat may not be widely known, yet has been widely used in LLMs, is the Gaussian Error Linear Units\nfunction [27]. Formally, we have GELU( x) := x \u00b7 1\n2 (1 + erf( x\u221a",
    "that may not be widely known, yet has been widely used in LLMs, is the Gaussian Error Linear Units\nfunction [27]. Formally, we have GELU( x) := x \u00b7 1\n2 (1 + erf( x\u221a\n2 )), where erf( x) := 2\u221a\u03c0\nRx\n0 e\u2212t2\ndt is the error\nfunction. The function can be understood as a smoother ReLU activation function and will be our focus in\nthe paper. In addition, M1 \u2208 Rd\u00d7dff , M2 \u2208 Rdff\u00d7d are linear transformation matrices, and b1, b2 are vectors.\nIn most practical cases, dff = 4d.\nCombining these blocks together, we define the function\nTransformer(S, j) := LN(FNN(LN(Attention(S, j)))). (7)\nNote that inputs for each function can be recovered from matrix S, index j, and outputs from the previous\nlayer functions. In currently employed transformer architectures, several of these building blocks are iterated\nfor a constant number of times. The output, i.e., the next predicted token, is sampled from the distribution",
    "for a constant number of times. The output, i.e., the next predicted token, is sampled from the distribution\nby further linear mapping the output vector to dimension dmodel and implementing the softmax function.\nConsidering the run time, recall that the length of the input sentence isN and the dimension of the embedded\nvectors is d. We summarize the time complexity as Table I.\nBlock Time complexity\nPreparation of Q, K, V Nd 2\nPreparation of QKT N2d\nPreparation of softmax(QKT /\n\u221a\nd)V =: Gsoft N2 + Nd2\nResidual connection LN(Gsoft\nj , Sj) 3 d\nFeed-forward NN FFN(LN(Gsoft\nj , Sj)) O(Nd2)\nTABLE I. Time complexity of transformer steps.\nThe time complexity of a constant number of iterations of the three main blocks is O(N2d + Nd2), which\nmainly comes from the self-attention matrix computation. If we only consider the 1-layer transformer, the\ntime complexity is O(Nd + d2), as we do not need to compute all N vectors that are needed for the second\nlayer self-attention block.",
    "time complexity is O(Nd + d2), as we do not need to compute all N vectors that are needed for the second\nlayer self-attention block.\nC. Quantum procedures\nTo encode the classical information into the quantum device, we use a standard input assumption in\nquantum algorithms literature, called the block encoding. Note that the encoding can be generalized to\n1 We note that this output vector can also be written asGsoft\nj (S) + Attention(0, 0, S)T \u02c6ej.\n6non-square matrix cases of arbitrary size by padding the matrix with zeros. Further, when we say we can\nconstruct or are given a block encoding unitary, it means we have access to the corresponding quantum\ncircuit, i.e., we can also implement the controlled, self-adjoint, and controlled self-adjoint of the circuit.\nDefinition 1 (Block encoding [12, 28]). We say a unitary UA is an (\u03b1, a, \u03f5)-encoding of matrix A \u2208 C2n\u00d72n\nif\n\u2225A \u2212 \u03b1(\u27e80a| \u2297In)UA(|0a\u27e9 \u2297In)\u2225 \u2264\u03f5. (8)",
    "Definition 1 (Block encoding [12, 28]). We say a unitary UA is an (\u03b1, a, \u03f5)-encoding of matrix A \u2208 C2n\u00d72n\nif\n\u2225A \u2212 \u03b1(\u27e80a| \u2297In)UA(|0a\u27e9 \u2297In)\u2225 \u2264\u03f5. (8)\nBy definition, one can see that \u03b1 \u2265 \u2225A\u2225, i.e., \u03b1 is at least the spectral norm of the block-encoded matrix.\nIn Appendix A, we describe some methods to construct the block encoding for certain kinds of matrices,\ne.g., sparse. Assuming the quantum random access memory [17] and quantum data structure [29], one can\nconstruct the block-encoding unitary for arbitrary matrix, paying the price of \u03b1 = \u2225A\u2225F , i.e., \u03b1 will be the\nFrobenius norm instead. Note that the Frobenius norm is strictly larger than the spectral norm.\nSince the outputs from each block of the transformer are vectors, we construct quantum circuits that\ngenerate quantum states corresponding to these vectors. We use the format of state preparation encoding\nintroduced in Ref. [30], yet change from L2 norm to L\u221e norm.",
    "generate quantum states corresponding to these vectors. We use the format of state preparation encoding\nintroduced in Ref. [30], yet change from L2 norm to L\u221e norm.\nDefinition 2 (State preparation encoding). We say a unitary U\u03c8 is an (\u03b1, a, \u03f5)-state-encoding of an n-qubit\nquantum state |\u03c8\u27e9 if\n\u2225|\u03c8\u27e9 \u2212\u03b1(\u27e80a| \u2297I)U\u03c8|0a+n\u27e9\u2225\u221e \u2264 \u03f5. (9)\nMore straightforwardly, the (\u03b1, a, \u03f5)-state-encoding U\u03c8 prepares the state\nU\u03c8|0\u27e9|0\u27e9 = 1\n\u03b1|0\u27e9|\u03c8\u2032\u27e9 +\np\n1 \u2212 \u03b12|1\u27e9|bad\u27e9,\nwhere \u2225|\u03c8\u2032\u27e9 \u2212 |\u03c8\u27e9\u2225\u221e \u2264 \u03f5 and |bad\u27e9 is an arbitrary quantum state. One can further prepare the state |\u03c8\u2032\u27e9 by\nusing O(\u03b1) times of amplitude amplification [31]. The state preparation encoding may also be understood\nas a block encoding of a C1\u00d72n\nmatrix.\nTo encode the classical coefficients into quantum states which will be used multiple times, we follow the\nresults in Ref. [32, 33].\nTheorem 2 (Quantum state preparation [32]) . For a given vector v \u2208 CN with \u2225v\u22252 = 1, one can prepare\na (1, 0, 0)-state-encoding Uv of state |v\u27e9 = PN",
    "results in Ref. [32, 33].\nTheorem 2 (Quantum state preparation [32]) . For a given vector v \u2208 CN with \u2225v\u22252 = 1, one can prepare\na (1, 0, 0)-state-encoding Uv of state |v\u27e9 = PN\ni=1 vi|i\u27e9 with depth O(N/ log N) without using ancilla qubits.\nOne can also achieve this with depth O(log N) with O(N) ancilla qubits.\nIn the following, we introduce some results on \u201clinear algebra\u201d of block-encoded matrices such as addition\nand multiplication. The first result is to achieve a linear combination of block-encoded matrices, which\nrequires the so-called state preparation pair.\nDefinition 3 (State preparation pair [12, 28]) . Let y \u2208 Cm and \u2225\u03b3\u2225 = 1 \u2264 \u03b2, the pair of unitaries\n(PL, PR) is called a (\u03b2, b, \u03f5)-state-preparation-pair if PL|0b\u27e9 = P2b\nk=1 ck|k\u27e9 and PR|0b\u27e9 = P2b\nk=1 dk|k\u27e9 such\nthat Pm\nk=1 |\u03b2(c\u2217\nkdk) \u2212 yk| \u2264\u03f5 and for all k \u2208 m + 1, . . . ,2b we have c\u2217\nkdk = 0.\nThis pair of circuits allows one to create a linear combination of matrices with given coefficients as the",
    "that Pm\nk=1 |\u03b2(c\u2217\nkdk) \u2212 yk| \u2264\u03f5 and for all k \u2208 m + 1, . . . ,2b we have c\u2217\nkdk = 0.\nThis pair of circuits allows one to create a linear combination of matrices with given coefficients as the\nnext lemma shows. We notice a typo in the original Lemma 52 in Ref. [12], and fix it as follows.\nLemma 1 (Linear combination of block-encoded matrices [12, 28]) . Let A = Pm\nk=1 ykAk be an s-qubit\noperator and \u03f5 > 0. Suppose that (PL, PR) is a (\u03b2, b, \u03f51)-state-preparation-pair for y, and that W =Pm\nk=1 |k\u27e9\u27e8k| \u2297Uk + ((I \u2212 Pm\nk=1 |k\u27e9\u27e8k|) \u2297 Ia \u2297 Is) is an s + a + b qubit unitary such that for all k \u2208 [m], the\nunitary Uk is an (\u03b1, a, \u03f52)-encoding of Ak. Then we can implement an (\u03b1\u03b2, a+ b, \u03b1\u03f51 + \u03b2\u03f52)-encoding of A,\nwith a single use of W, PR and P\u2020\nL.\nThe second result is to achieve a multiplication of block-encoded matrices.\n7Lemma 2 (Product of block-encoded matrices [12, 28]) . If U is an (\u03b1, a, \u03b4)-encoding of an s-qubit operator",
    "L.\nThe second result is to achieve a multiplication of block-encoded matrices.\n7Lemma 2 (Product of block-encoded matrices [12, 28]) . If U is an (\u03b1, a, \u03b4)-encoding of an s-qubit operator\nA, and V is a (\u03b2, b, \u03f5)-encoding of an s-qubit operator B, then (Ib \u2297 U)(Ia \u2297 V ) is an (\u03b1\u03b2, a+ b, \u03b1\u03f5+ \u03b2\u03b4)-\nencoding of AB.\nGiven the block-encoding, we can implement polynomial functions on singular values of block-encoded\nmatrices (or eigenvalues for Hermitian matrices) using the quantum singular value transformation (QSVT)\nmethod.\nTheorem 3 (Polynomial eigenvalue transformation [12]). Let \u03b4 >0. Given U that is an (\u03b1, a, \u03f5)-encoding\nof a Hermitian matrix A, and a real \u2113-degree function f(x) with |f(x)| \u2264 1\n2 for x \u2208 [\u22121, 1], one can\nprepare a (1, a+ n + 4, 4\u2113\np\n\u03f5/\u03b1 + \u03b4)-encoding of f(A/\u03b1) by using O(\u2113) queries to U and O(\u2113(a + 1)) one-\nand two-qubit quantum gates. The description of the quantum circuit can be computed classically in time\nO(poly(d, log(1/\u03b4))).",
    "and two-qubit quantum gates. The description of the quantum circuit can be computed classically in time\nO(poly(d, log(1/\u03b4))).\nAn additional point to note is that for the classical case, they consider the row vector as described\npreviously. However, for the quantum case, we consider the column vector, i.e., the quantum state. This\nsmall difference can be handled by implementing the self-adjoint of the unitary.\nIII. PROBLEM FORMULATIONS\nHere, we describe our assumptions and the problem statements that are considered for the solving on\nquantum computers. Recall that in this paper, we focus on the inference and assume the training process\nhas already been achieved. The classical problems assume memory access to the inputs such as the sentence\nand the query, key, and value matrices. The quantum algorithms change this input assumption to a block\nencoding input assumption. The dimensions of N and d can be achieved by padding with zeros.",
    "and the query, key, and value matrices. The quantum algorithms change this input assumption to a block\nencoding input assumption. The dimensions of N and d can be achieved by padding with zeros.\nDefinition 4 (Input assumption) . We assume N = 2 n and d = 2 log d for n, log d \u2208 N+. For the input\nsequence S \u2208 RN\u00d7d, we assume given access to a quantum circuit US which is an (\u03b1s, as, \u03f5s)-encoding of\nS. For matrices Wq, Wk, Wv \u2208 Rd\u00d7d, assume given access to quantum circuits UWq , UWk , and UWv that\nare (\u03b1w, aw, \u03f5w)-encodings of Wq, Wk and Wv respectively. For the feed-forward neural network, we assume\n(\u03b1m, am, \u03f5m)-encodings UM1 and UM2 of two weight matrices M1 \u2208 R N1\u00d7N and M2 \u2208 R N2\u00d7N1 .\nWe reformulate the classical problems to the quantum version based on this input assumption.\nProblem 1 (Quantum self-attention). Assume the input assumption as in Definition 4. Define Q := SWq,\nK := SWk, and V := SWv. Let the current focused token be j \u2208 [N], the task is to construct a block-encoding",
    "K := SWk, and V := SWv. Let the current focused token be j \u2208 [N], the task is to construct a block-encoding\nof the matrix G such that\nGj\u22c6 = Gsoft\nj :=\n\u0000\nsoftmax(QKT /\u03b10)V\n\u0001\nj\u22c6, (10)\nwhere \u03b10 = \u03b12\ns\u03b12\nw. For the masked self-attention, change Gsoft to softmax(QKT /\u03b10 + M)V , where M is the\nmasked matrix as Eq. (4).\nNote that we change the scaling coefficient \u03b10 for the quantum case. Details of the explanation can be\nfound in Section IV C.\nProblem 2 (Quantum residual connection with layer normalization) . Assume the input assumption as in\nDefinition 4. Assume given access to an (\u03b1g, ag, \u03f5g)-encoding of the self-attention Gsoft as Eq. (10). Let the\ncurrent query token be the j-th token. Construct a state preparation encoding of the state\ndX\nk=1\nLN\u03b3,\u03b2 (Gsoft\nj , Sj)k|k\u27e9, (11)\nwhere LN\u03b3,\u03b2 is as Eq. (5). Here, \u03b3 = 1/\n\u221a\nd and \u03b2 = 0.\n8Note that standardization rescales the \u21132-norm of the vector to be\n\u221a\nd. By taking \u03b3 = 1/\n\u221a\nd and \u03b2 = 0,",
    "dX\nk=1\nLN\u03b3,\u03b2 (Gsoft\nj , Sj)k|k\u27e9, (11)\nwhere LN\u03b3,\u03b2 is as Eq. (5). Here, \u03b3 = 1/\n\u221a\nd and \u03b2 = 0.\n8Note that standardization rescales the \u21132-norm of the vector to be\n\u221a\nd. By taking \u03b3 = 1/\n\u221a\nd and \u03b2 = 0,\nthe \u21132-norm will be 1. We consider this case to simplify our discussion. We also provide a general discussion\nin Appendix F.\nProblem 3 (Quantum two-layer feedforward network). Assume the input assumption as in Definition 4.\nGiven an (\u03b1, a, \u03f5)-state-encoding U\u03c8 of an n-qubit state |\u03c8\u27e9 = PN\nk=1 \u03c8k|k\u27e9, where {\u03c8k} are real and \u2225\u03c8\u22252 = 1,\nand an activation function \u03c3, prepare a state encoding of the quantum state |\u03d5\u27e9\n|\u03d5\u27e9 = 1\nC\nN2X\nk=1\n\u0000\nM2 \u00b7 \u03c3(M1 \u00b7 \u03c8)\n\u0001\nk|k\u27e9, (12)\nwhere C is the normalization factor.\nIV. MAIN RESULTS\nIn this section, we present our main technical contributions. The first contribution is to show how to\nimplement the element-wise functions to a block-encoded matrix, which plays an essential role in the quantum",
    "implement the element-wise functions to a block-encoded matrix, which plays an essential role in the quantum\nself-attention block. To achieve this, we also show how to perform the Hadamard product of block-encoded\nmatrix. The second contribution is to clearly state the conversion between state preparation encoding and\nmatrix block encoding, based on the previous works about nonlinear amplitude transformation [30, 34]. This\nensures we can implement the complex transformer architecture coherently on the quantum computer. Based\non these methods and some more tricks, we describe how we may implement the quantum self-attention,\nresidual connection and layer normalization, and the FNN blocks on the quantum computer.\nA. Element-wise function of block-encoded matrices\nIn this section, we show an essential building block for our algorithm. For a function f : R \u2192 R and\na matrix A \u2208 C2n\u00d72n\n, the task is to apply the element-wise operation f \u25e6 (A). In a classical or quantum",
    "a matrix A \u2208 C2n\u00d72n\n, the task is to apply the element-wise operation f \u25e6 (A). In a classical or quantum\nquery model, the solution is to apply the function after each particular element is queried. However, here we\ndo not work in the query model. The matrix A is accessed via a block encoding, which includes the query\nmodel, but also includes the use of other input models such as input from a preceding subroutine.\nThe key idea of our subroutines is as follows, see below for the formal results. Assume that f in some\nrange admits a polynomial approximation g with some degree dpoly and some point-wise error, i.e, f(x) \u2248\ng(x) = Pdpoly\nk=0 ckxk. For each entry of the matrix inside the range, it holds that f(Aij) \u2248 g(Aij) and thus\n[f \u25e6 (A)]ij \u2248 [g \u25e6 (A)]ij. We can express the entry as [ g \u25e6 (A)]ij = Pdpoly\nk=0 ckAk\nij = Pdpoly\nk=0 ck(A\u25e6k)ij, using\nthe k-th Hadamard product of the matrix with itself, A\u25e6k. Furthermore, there exist a matrix P such that",
    "k=0 ckAk\nij = Pdpoly\nk=0 ck(A\u25e6k)ij, using\nthe k-th Hadamard product of the matrix with itself, A\u25e6k. Furthermore, there exist a matrix P such that\nA\u25e6k = [P A\u2297k PT ]block, where the subscript \u201cblock\u201d indicates that we choose the correct block of the matrix\nP A\u2297k PT . Hence, we find that\n[f \u25e6 (A)]ij \u2248\ndpolyX\nk=0\nck\n\u0002\n[P A\u2297k PT ]block\n\u0003\nij. (13)\nIn summary, the quantum algorithm uses a tensor-product of matrices, permutation matrices, linear combi-\nnation of matrices, and polynomial approximation to construct an elementwise application of a function to\nthe entries of a matrix.\nWe start with a lemma about the max-norm of a block-encoding.\nLemma 3. If U is an (\u03b1, a, \u03f5)-encoding of matrix A \u2208 C2n\u00d72n\n, we have\nmax\ni,j\u2208[2n]\n|\u03b1(\u27e80a| \u2297 \u27e8i|)U(|0a\u27e9 \u2297 |j\u27e9) \u2212 Aij| \u2264\u03f5. (14)\n9Proof. Let B = A \u2212 \u03b1(\u27e80a| \u2297I)U(|0a\u27e9 \u2297I), which is a complex matrix. By definition,\n\u2225B\u2225 = \u2225A \u2212 \u03b1(\u27e80a| \u2297I)U(|0a\u27e9 \u2297I)\u2225 \u2264\u03f5.\nBy Lemma S4, we have max i,j |Bij| \u2264\u03f5.",
    "9Proof. Let B = A \u2212 \u03b1(\u27e80a| \u2297I)U(|0a\u27e9 \u2297I), which is a complex matrix. By definition,\n\u2225B\u2225 = \u2225A \u2212 \u03b1(\u27e80a| \u2297I)U(|0a\u27e9 \u2297I)\u2225 \u2264\u03f5.\nBy Lemma S4, we have max i,j |Bij| \u2264\u03f5.\nAs seen from the qualitative discussion above, we have to be able to construct the Hadamard product\nbetween matrices. Here, we consider the general case of two different matrices.\nTheorem 4 (Hadamard product of block-encoded matrices) . With n \u2208 N and N = 2n, consider matrices\nA1, A2 \u2208 CN\u00d7N , and assume that we have an (\u03b1, a, \u03b4)-encoding of matrix A1 and (\u03b2, b, \u03f5)-encoding of matrix\nA2. We can construct an (\u03b1\u03b2, a+ b + n, \u03b1\u03f5+ \u03b2\u03b4)-encoding of matrix A1 \u25e6 A2.\nProof. For simplicity, we first consider the perfect case without input block-encoding errors. Let U1 and U2\nbe the (\u03b1, a,0)- or (\u03b2, b,0)-encoding unitary of A1 and A2, respectively. Note that\n(\u27e80a+b|)U1 \u2297 U2(|0a+b\u27e9) = 1\n\u03b1\u03b2 A1 \u2297 A2. (15)\nLet P\u2032 = PN\u22121\ni=0 |i\u27e9\u27e8i| \u2297 |0\u27e9\u27e8i|. As shown in Ref. [35], P\u2032(A1 \u2297 A2)P\u2032\u2020 = (A1 \u25e6 A2) \u2297 |0\u27e9\u27e80|. However, note",
    "(\u27e80a+b|)U1 \u2297 U2(|0a+b\u27e9) = 1\n\u03b1\u03b2 A1 \u2297 A2. (15)\nLet P\u2032 = PN\u22121\ni=0 |i\u27e9\u27e8i| \u2297 |0\u27e9\u27e8i|. As shown in Ref. [35], P\u2032(A1 \u2297 A2)P\u2032\u2020 = (A1 \u25e6 A2) \u2297 |0\u27e9\u27e80|. However, note\nthat P\u2032 is not a unitary. Instead, we consider P = PN\u22121\ni,j=0 |i\u27e9\u27e8i| \u2297 |i \u2295 j\u27e9\u27e8j|, which can be easily constructed\nby using n CNOT gates, i.e., one CNOT gate between each pair of qubits consisting of one qubit from the\nfirst register and the corresponding qubit from the second register. By direct computation, we have\n(In \u2297 \u27e80n|)P(A1 \u2297 A2)P\u2020(In \u2297 |0n\u27e9) = A1 \u25e6 A2. (16)\nTherefore,\n(In \u2297 \u27e80n+a+b|)\n\u0000\n(P \u2297 Ia+b)U1 \u2297 U2(P\u2020 \u2297 Ia+b)\n\u0001\n(In \u2297 |0n+a+b\u27e9) = 1\n\u03b1\u03b2 A1 \u25e6 A2. (17)\nNow we consider the error from the input block encodings. Write \u00afA1 := \u03b1\u27e80a|U1|0a\u27e9 and \u00afA2 := \u03b2\u27e80b|U2|0b\u27e9.\nLet B1 = A1 \u2212 \u00afA1 and B2 = A2 \u2212 \u00afA2. By definition, \u2225B1\u2225 \u2264\u03b4, \u2225B2\u2225 \u2264\u03f5. The error can be bounded by\n\r\rA1 \u25e6 A2 \u2212 \u03b1\u03b2(\u27e80n+a+b|)\n\u0000\n(P \u2297 Ia+b)U1 \u2297 U2(P\u2020 \u2297 Ia+b)\n\u0001\n(|0n+a+b\u27e9)\n\r\r\n\u2264\n\r\rA1 \u25e6 A2 \u2212 \u03b1\u03b2\u27e80n|\n\u0000\nP(\u27e80a+b|U1 \u2297 U2|0a+b\u27e9)P\u2020\u0001\n|0n\u27e9\n\r\r\n\u2264\n\r\rA1 \u25e6 A2 \u2212 \u27e80n|\n\u0000\nP \u00afA1 \u2297 \u00afA2P\u2020\u0001\n|0n\u27e9",
    "A1 \u25e6 A2 \u2212 \u03b1\u03b2(\u27e80n+a+b|)\n\u0000\n(P \u2297 Ia+b)U1 \u2297 U2(P\u2020 \u2297 Ia+b)\n\u0001\n(|0n+a+b\u27e9)\n\r\r\n\u2264\n\r\rA1 \u25e6 A2 \u2212 \u03b1\u03b2\u27e80n|\n\u0000\nP(\u27e80a+b|U1 \u2297 U2|0a+b\u27e9)P\u2020\u0001\n|0n\u27e9\n\r\r\n\u2264\n\r\rA1 \u25e6 A2 \u2212 \u27e80n|\n\u0000\nP \u00afA1 \u2297 \u00afA2P\u2020\u0001\n|0n\u27e9\n\r\r\n\u2264\n\r\rA1 \u25e6 A2 + \u27e80n|\n\u0000\nP A1 \u2297 \u00afA2P\u2020\u0001\n|0n\u27e9 \u2212 \u27e80n|\n\u0000\nP A1 \u2297 \u00afA2P\u2020\u0001\n|0n\u27e9 \u2212 \u27e80n|\n\u0000\nP \u00afA1 \u2297 \u00afA2P\u2020\u0001\n|0n\u27e9\n\r\r\n\u2264\n\r\rA1 \u25e6 A2 \u2212 \u27e80n|\n\u0000\nP A1 \u2297 \u00afA2P\u2020\u0001\n|0n\u27e9\n\r\r +\n\r\r\u27e80n|\n\u0000\nP A1 \u2297 \u00afA2P\u2020\u0001\n|0n\u27e9 \u2212 \u27e80n|\n\u0000\nP \u00afA1 \u2297 \u00afA2P\u2020\u0001\n|0n\u27e9\n\r\r\n\u2264\n\r\r\u27e80n|\n\u0000\nP A1 \u2297 B2P\u2020\u0001\n|0n\u27e9\n\r\r +\n\r\r\u27e80n|\n\u0000\nP B1 \u2297 \u00afA2P\u2020\u0001\n|0n\u27e9\n\r\r\n\u2264 \u03b1\u03f5 + \u03b2\u03b4. (18)\nThe previous lemma can be implemented iteratively. Given an ( \u03b1, a, \u03f5)-encoding of matrix A, for j \u2208 N,\none can construct an (1 , ja+ (j \u2212 1)n, j\u03f5/\u03b1)-encoding of matrix ( A/\u03b1)\u25e6j := ( A/\u03b1) \u25e6 (A/\u03b1) \u25e6 \u00b7\u00b7\u00b7 \u25e6(A/\u03b1)\ncontaining j \u2212 1 Hadamard products among j copies of matrix A/\u03b1. Hence, we can implement polynomials\non the entries of A/\u03b1.\nTheorem 5 (Element-wise polynomial function of block-encoded matrix) . Let n, k\u2208 N . Given access to\nan (\u03b1, a, \u03f5) block-encoding of a matrix A \u2208 C2n\u00d72n\nand an \u2113-degree polynomial function f\u2113(x) = P\u2113",
    "Theorem 5 (Element-wise polynomial function of block-encoded matrix) . Let n, k\u2208 N . Given access to\nan (\u03b1, a, \u03f5) block-encoding of a matrix A \u2208 C2n\u00d72n\nand an \u2113-degree polynomial function f\u2113(x) = P\u2113\nj=0 cjxj,\nc0, cj \u2208 C for j \u2208 [l], one can construct a (C, b, \u03b3)-encoding of f\u2113 \u25e6 (A/\u03b1) by using O(\u21132) times the input\nunitary, where C := P\u2113\nj=0 |cj|, b := \u2113a + (\u2113 \u2212 1)n + 2 log\u2113, and \u03b3 := \u03f5\n\u03b1 \u00b7 (P\u2113\nj=0 |cj|j).\n10Proof. We first consider the perfect case, i.e., \u03f5 = 0. To achieve this implementation, we construct two\nstate-preparation unitaries, which act on \u2308log(\u2113 + 1)\u2309 qubits such that\nU1 : |0\u2308log(\u2113+1)\u2309\u27e9 \u2192 1\u221a\nC\n\u2113X\nj=0\nq\n|cj||j\u27e9, (19)\nU2 : |0\u2308log(\u2113+1)\u2309\u27e9 \u2192 1\u221a\nC\n\u2113X\nj=0\nq\n|cj|ei\u03b8j |j\u27e9, (20)\nwhere C = P\u2113\nj=0 |cj| and |cj|ei\u03b8j = cj. By Theorem 2, U1 and U2 can be prepared with depth O(\u2113) using only\nelementary quantum gates. Therefore, ( U1, U2) is a ( C, 2 log\u2113, 0) state-preparation pair of ( c0, c1, . . . , c\u2113).",
    "elementary quantum gates. Therefore, ( U1, U2) is a ( C, 2 log\u2113, 0) state-preparation pair of ( c0, c1, . . . , c\u2113).\nLet Uj be the (1, ja+ (j \u2212 1)n, 0)-encoding of (A/\u03b1)\u25e6j, which we construct by iteratively applying Theo-\nrem 4. Then, we construct a ( \u2113a+\u2113n+2 log \u2113)-qubit unitary W = P\u2113\nj=0 |j\u27e9\u27e8j|\u2297 Vj +(I2 log\u2113 \u2212P\u2113\nj=0 |j\u27e9\u27e8j|)\u2297\nI\u2113a+\u2113n. We are in the setting of the linear combination of block encodings and by Lemma 1, we can implement\na (C, \u2113a+ (\u2113 \u2212 1)n + 2 log\u2113, 0)-encoding of f\u2113 \u25e6 (A/\u03b1).\nNow we perform the error analysis. As mentioned, for each ( A/\u03b1)\u25e6j, the error is bounded by j\u03f5/\u03b1.\nSumming up these errors, the error of f\u2113 \u25e6 (A/\u03b1) can be bounded by \u03f5\n\u03b1 \u00b7 (P\u2113\nj=0 |cj|j) =: \u03b3.\nHow to use polynomial functions to approximate many useful functions has been well studied in the field of\napproximation theory. Those results have also been utilized in the quantum computing field for QSVT-based\nquantum algorithms via quantum signal processing [11].",
    "approximation theory. Those results have also been utilized in the quantum computing field for QSVT-based\nquantum algorithms via quantum signal processing [11].\nB. Conversion between state preparation encoding and matrix block encoding\nTypically for each block in the transformer, the input is a vector \u03c8 and the output is another vector f(\u03c8)\nin the same dimension with some nonlinear transformations. As the quantum analog, the question becomes\ngiven a state-preparation unitary of some input state |\u03c8\u27e9, output a state-preparation unitary of the state\n|f(\u03c8)\u27e9.\nTo achieve this, we use the diagonal block encoding developed in the context of the nonlinear amplitude\ntransformation method, which has been introduced in Ref. [30, 34]. The key insight of the nonlinear amplitude\ntransformation is that it can convert a state preparation encoding as in Definition 2 to a matrix block encoding\nas Definition 1. Then, by Theorem 3 one can implement polynomial functions onto these amplitudes. For",
    "as Definition 1. Then, by Theorem 3 one can implement polynomial functions onto these amplitudes. For\nour discussion, we directly describe the robust version, which is a straightforward generalization of previous\nworks. The proof is provided in Appendix B.\nTheorem 6 (Robust amplitude encoding [30, 34]) . Given an (\u03b1, a, \u03f5)-state-encoding U\u03c8 of an n-qubit state\n|\u03c8\u27e9 = PN\nj=1 \u03c8j|j\u27e9, where {\u03c8j} are real and \u2225\u03c8\u22252 = 1 , one can construct an (\u03b1, 2a + n + 2, \u03f5)-encoding\nof the diagonal matrix A = diag( \u03c81, . . . , \u03c8N ) with O(n) circuit depth and O(1) queries to controlled- U\nand controlled-U\u2020. One can also construct an (\u03b12, 3a + 2n + 2, 3\u03f5)-encoding of diagonal matrix Aabs =\ndiag(\u03c82\n1, . . . , \u03c82\nN ).\nThe reason why we slightly changed the definition of state preparation encoding compared to Ref. [30], i.e.,\nfrom L2 norm to L\u221e norm is that after robust amplitude encoding, the L\u221e distance between the target state",
    "from L2 norm to L\u221e norm is that after robust amplitude encoding, the L\u221e distance between the target state\n|\u03c8\u27e9 and exact preparable state |\u03c8\u2032\u27e9 is directly the upper bound of \u2225diag(\u03c81, . . . , \u03c8N ) \u2212 diag(\u03c8\u2032\n1, . . . , \u03c8\u2032\nN )\u2225.\nAfter implementing functions with QSVT, one needs to convert the block-encoding back to the state-\nencoding. This can be achieved by either the uniform-weighted [34] or the importance-weighted [30] method.\nThe first one is more general, yet the latter one can achieve a much better, i.e., up to exponentially better,\ndependency on the state dimension. A point to note is about the error analysis. We have the error bound\nin matrix norm for block-encoding, which is also an upper bound for each matrix element difference, as\nLemma 3. However, in general, the column/row of the block-encoded matrix is not normalized in the L2\nnorm, so we also need to consider the influence of the normalization factor. We prove the following lemma,",
    "norm, so we also need to consider the influence of the normalization factor. We prove the following lemma,\nwhere the proof is provided in Appendix D.\n11Lemma 4. For two d-dimensional vectors \u03c8 = (\u03c81, . . . , \u03c8d) and \u03c8\u2032 = (\u03c8\u2032\n1, . . . , \u03c8\u2032\nd), if |\u03c8j \u2212 \u03c8\u2032\nj| \u2264\u03f5 for each\nj \u2208 [d], we have\n\r\r\r 1\nC \u03c8 \u2212 1\nC\u2032 \u03c8\u2032\n\r\r\r\n\u221e\n\u2264 (\n\u221a\nd + 1)\u03f5\nC +\ns\n2\u03f5\n\u221a\nd\nC = O\n\uf8eb\n\uf8ed\ns\n\u03f5\n\u221a\nd\nC\n\uf8f6\n\uf8f8, (21)\nwhere C = \u2225\u03c8\u22252 and C\u2032 = \u2225\u03c8\u2032\u22252.\nAs an example, one can easily see the following stands using lemma Lemma 4.\nRemark 1. Given an (\u03b1, a, \u03f5)-encoding UA of a matrix A \u2208 Cd\u00d7d, for Ui : |0\u27e9 \u2192 |i\u27e9 where i \u2208 [d], UA(Ui\u2297Ia)\nis a (O(C/\u03b1), a,O((\u03f5\n\u221a\nd/C)\n1\n2 ))-state-encoding of 1\nC\nPd\nj=1 Aji|j\u27e9, where C = \u2225A\u22c6i\u22252.\nC. Quantum self-attention\nIn this section, we describe how to achieve the quantum self-attention block. We are given the block\nencoding of matrices as input and let j-th token be the current query vector, the output is a block encoding",
    "encoding of matrices as input and let j-th token be the current query vector, the output is a block encoding\nunitary of a matrix whose j-th row is the same as the output of the classical transformer. We divide it into\ntwo parts: the first part is to achieve the softmax function, where we use the element-wise function method\nas Theorem 5; the second part is to achieve the remaining procedures, where we use the amplitude encoding\nas Theorem 6. The key insight for achieving the softmax function is that it can also be understood that we\nfirst implement exp \u25e6(QKT /\u03b10), then multiply with different coefficients (normalization) for each row.\nFor quantum self-attention, we set the scaling factor \u03b10 = \u03b12\ns\u03b12\nw for the following reasons. The first is that\nthe 1/\n\u221a\nd is chosen somehow in a heuristic sense, and there are some classical works considering different\nscaling coefficients [23, 24]. The second, which is more important, is that the quantum input assumption",
    "scaling coefficients [23, 24]. The second, which is more important, is that the quantum input assumption\nusing the block encoding format naturally contains the normalization factor \u03b1 which plays a similar role to\n1/\n\u221a\nd. Therefore, for the quantum case in the context of our work, it suffices to use \u03b1 directly.\nTheorem 7 (Quantum softmax for self-attention). Given an (\u03b1, a, \u03f5)-encoding UA of a matrix A \u2208 RN\u00d7N ,\na positive integer d \u2208 N+, and an index j \u2208 [N], one can prepare a\n\u0000\n1, O(\u2113(a + n)), O\n\u0000 4\nq\nN\nZj\n\u221a\u03f5\n\u0001\u0001\n-encoding\nof the matrix\ndiag(softmax(A/\u03b1)j1, . . . ,softmax(A/\u03b1)jN ),\nby using UA for O\n\u0000 \u21132\n\u221a\nZ\n\u0001\ntimes, where Zj = PN\nk=1 exp \u25e6(A/\u03b1)jk, and \u2113 = O\n\u0000\nn log(1\n\u03f5 )\n\u0001\n.\nProof. We first construct the block encoding of exp \u25e6( A\n2\u03b1 ). This can be achieved with Theorem 5 and\nLemma S8. There are two error terms in this step. Note that by the definition of Definition 1, |A/\u03b1|jk \u2264 1\nfor j, k\u2208 [N]. The first term comes from the intrinsic error of block encodings, and the second is from",
    "for j, k\u2208 [N]. The first term comes from the intrinsic error of block encodings, and the second is from\nthe polynomial approximation. Denote Uf\u25e6(A) as the constructed block encoding unitary. By Theorem 5,\nUf\u25e6(A) is a ( Cf , bf , \u03b3f )-encoding of f\u2113 \u25e6 (A), where Cf = P\u2113\nj=0 1/j!, bf = \u2113a + (\u2113 \u2212 1)n + 2 log\u2113, and\n\u03b3f = \u03f5\n\u03b1 \u00b7 P\u2113\nj=1 1/(j \u2212 1)!. By triangle inequality, we have\n\r\r\r\rexp \u25e6\n\u0010 A\n2\u03b1\n\u0011\n\u2212 Cf \u27e80bf |Uf\u25e6(A)|0bf \u27e9\n\r\r\r\r\n=\n\r\r\r\rexp \u25e6\n\u0010 A\n2\u03b1\n\u0011\n\u2212 f\u2113 \u25e6 (A) + f\u2113 \u25e6 (A) \u2212 Cf \u27e80bf |Uf\u25e6(A)|0bf \u27e9\n\r\r\r\r\n\u2264\n\r\r\r\rexp \u25e6\n\u0010 A\n2\u03b1\n\u0011\n\u2212 f\u2113 \u25e6 (A)\n\r\r\r\r +\n\r\rf\u2113 \u25e6 (A) \u2212 Cf \u27e80bf |Uf\u25e6(A)|0bf \u27e9\n\r\r\n\u2264\n\r\r\r\rexp \u25e6\n\u0010 A\n2\u03b1\n\u0011\n\u2212 f\u2113 \u25e6 (A)\n\r\r\r\r + \u03b3f . (22)\n12Note that we can bound for each element between exp \u25e6( A\n2\u03b1 ) and f\u2113 \u25e6 (A) with error \u03b4, which comes from\nthe polynomial approximation. By the norm inequality between spectral and Frobenius norm, we have\n\r\r\r\rexp \u25e6\n\u0010 A\n2\u03b1\n\u0011\n\u2212 fk \u25e6 (A)\n\r\r\r\r \u2264\n\r\r\r\rexp \u25e6\n\u0010 A\n2\u03b1\n\u0011\n\u2212 fk \u25e6 (A)\n\r\r\r\r\nF\n=\n\u0012X\nj,k\n\f\f\fexp \u25e6\n\u0010 A\n2\u03b1\n\u0011\njk\n\u2212 f\u2113 \u25e6 (A)jk\n\f\f\f\n2\u00131\n2\n\u2264\n\u0000\nN2\u03b42\u00011\n2 \u2264 N\u03b4. (23)",
    "exp \u25e6\n\u0010 A\n2\u03b1\n\u0011\n\u2212 fk \u25e6 (A)\n\r\r\r\r \u2264\n\r\r\r\rexp \u25e6\n\u0010 A\n2\u03b1\n\u0011\n\u2212 fk \u25e6 (A)\n\r\r\r\r\nF\n=\n\u0012X\nj,k\n\f\f\fexp \u25e6\n\u0010 A\n2\u03b1\n\u0011\njk\n\u2212 f\u2113 \u25e6 (A)jk\n\f\f\f\n2\u00131\n2\n\u2264\n\u0000\nN2\u03b42\u00011\n2 \u2264 N\u03b4. (23)\nTo make the error bounded \u03f5, we set \u2113 = O\n\u0000\nlog(N\n\u03f5 )\n\u0001\n= O\n\u0000\nn log(1\n\u03f5 )\n\u0001\n. By Lemma S4, we have\nmax\nj,k\u2208[N]\n\f\f\fexp \u25e6\n\u0010 A\n2\u03b1\n\u0011\njk\n\u2212 Cf (\u27e80bf |\u27e8i|)Uf\u25e6(A)(|0bf \u27e9|j\u27e9)\n\f\f\f \u2264\n\r\r\r\rexp \u25e6\n\u0010 A\n2\u03b1\n\u0011\n\u2212 Cf \u27e80bf |Uf\u25e6(A)|0bf \u27e9\n\r\r\r\r\n\u2264 \u03f5 + \u03b3f = O(\u03f5). (24)\nNote that exp \u25e6( A\n2\u03b1 )jk = exp \u25e6( A\n2\u03b1 )T\nkj. For index j \u2208 [N], let Uj : |0\u27e9 \u2192 |j\u27e9. Unitary U\u2020\nf\u25e6(A)(I \u2297 Uj) and\namplitude amplification prepare a state that is close to the target state\n|Aj\u27e9 := 1p\nZj\nNX\nk=1\nexp \u25e6\n\u0010 A\n2\u03b1\n\u0011\njk\n|k\u27e9, (25)\nwhere Zj = PN\nk=1 exp \u25e6(A/\u03b1)jk is the normalization factor of softmax function for the j-th row. By\nLemma 4, the L\u221e distance between the prepared state and the target state is O\n\u0000\n(\u03f5\np\nN/Zj)\n1\n2\n\u0001\n. There-\nfore, U\u2020\nf\u25e6(A)(I \u2297 Uj) is an\n\u0000\nO(Cf /\np\nZj), bf , O\n\u0000\n(\u03f5\np\nN/Zj)\n1\n2\n\u0001\u0001\n-state-encoding of state |Aj\u27e9. By amplitude",
    "\u0000\n(\u03f5\np\nN/Zj)\n1\n2\n\u0001\n. There-\nfore, U\u2020\nf\u25e6(A)(I \u2297 Uj) is an\n\u0000\nO(Cf /\np\nZj), bf , O\n\u0000\n(\u03f5\np\nN/Zj)\n1\n2\n\u0001\u0001\n-state-encoding of state |Aj\u27e9. By amplitude\namplification [31], one can prepare a (1 , bf , O\n\u0000\n(\u03f5\np\nN/Z)\n1\n2\n\u0001\n-state-encoding of state |Ai\u27e9 using O(Cf /\n\u221a\nZ)\ntimes of U\u2020\nf\u25e6(A)(I \u2297Ui). By Theorem 6, this can be converted to a (1 , 2n+3bf +2, O\n\u0000\n(\u03f5\np\nN/Z)\n1\n2\n\u0001\n)-encoding\nof diag(softmax(A/\u03b1)j1, . . . ,softmax(A/\u03b1)jN ).\nThen we use the quantum softmax function to implement the block encoding of the self-attention matrix,\nas shown in the following theorem.\nTheorem 8 (Quantum self-attention). Consider the setting as in Problem 1. Let \u03b10 = \u03b12\ns\u03b12\nw. For the index\nj \u2208 [N], one can construct an\n\u0000\n\u03b1s\u03b1w\n\u221a\nN, O(\u2113(n + as + aw)), O\n\u0000\u221a\nN\n4\nq\nN\nZj\n\u221a\u03f5s + \u03f5w\n\u0001\u0001\n-encoding of a matrix\nG such that Gj\u22c6 = Gsoft\nj := (softmax\n\u0010\nQKT\n\u03b10\n\u0011\nV )j\u22c6, by using O( \u21132\n\u221a\nZj\n) times of US, UWq , UWk and UWv , where\nZj = PN\nk=1 exp \u25e6(QKT /\u03b10)jk, and \u2113 = O(n log( 1\n\u03f5s+\u03f5w\n)).",
    "-encoding of a matrix\nG such that Gj\u22c6 = Gsoft\nj := (softmax\n\u0010\nQKT\n\u03b10\n\u0011\nV )j\u22c6, by using O( \u21132\n\u221a\nZj\n) times of US, UWq , UWk and UWv , where\nZj = PN\nk=1 exp \u25e6(QKT /\u03b10)jk, and \u2113 = O(n log( 1\n\u03f5s+\u03f5w\n)).\nProof. In the first step, we construct the block encoding of matrix QKT and V. Note that for a real matrix\nM and its block encoding unitary UM , U\u2020\nM is the block encoding of MT . By Lemma 2, one can construct an\n(\u03b10, a0, \u03f50)-encoding UQKT of QKT , where \u03b10 := \u03b12\ns\u03b12\nw, a0 = 2as + 2aw, and \u03f50 = 2\u03b1s\u03b12\nw\u03f5s + 2\u03b12\ns\u03b1w\u03f5w. One\ncan also construct an (\u03b1v, av, \u03f5v)-encoding UV of V , where \u03b1v = \u03b1s\u03b1w, av = as + aw, and \u03f5v = \u03b1s\u03f5w + \u03b1w\u03f5s.\nBy Theorem 7, using UQKT for O\n\u0000Cf \u21132\n\u221a\nZj\n\u0001\ntimes, where Zj = PN\nk=1 exp \u25e6(QKT /\u03b10)jk, \u2113 = O\n\u0000\nn log( 1\n\u03f5s+\u03f5w\n)\n\u0001\n,\nCf = P\u2113\nj=0\n1\nj! , bf = \u2113a0 + (\u2113 \u2212 1)n + 2 log\u2113, and \u03b3f = \u03f50\n\u03b10\n\u00b7 P\u2113\nj=1\n1\n(j\u22121)! = O(\u03f5s + \u03f5w), one can prepare a\n(1, 2n + 3bf + 2, O\n\u0000\n((\u03f5s + \u03f5w)\np\nN/Z)\n1\n2\n\u0001\n)-encoding of the matrix\ndiag\n\u0000\nsoftmax(QKT /\u03b10)j1, . . . ,softmax(QKT /\u03b10)jN\n\u0001\n,",
    "\u03b10\n\u00b7 P\u2113\nj=1\n1\n(j\u22121)! = O(\u03f5s + \u03f5w), one can prepare a\n(1, 2n + 3bf + 2, O\n\u0000\n((\u03f5s + \u03f5w)\np\nN/Z)\n1\n2\n\u0001\n)-encoding of the matrix\ndiag\n\u0000\nsoftmax(QKT /\u03b10)j1, . . . ,softmax(QKT /\u03b10)jN\n\u0001\n,\nwhose diagonal elements correspond to the j-th row of softmax( QKT /\u03b10). By Lemma 3, the absolute\ndifference for each element is also bounded by O\n\u0000\n((\u03f5s + \u03f5w)\np\nN/Z)\n1\n2\n\u0001\n. Let this block-encoding unitary be\nUf(QKT ).\n13Finally, we need to do the matrix multiplication with V . To achieve this, we first need a projection\noperator PN\nk=1|j\u27e9\u27e8k| to shift the diagonal elements back to the j-th row. For index j \u2208 [N], let Uj :\n|0\u27e9 \u2192 |j\u27e9. Consider unitary HnU\u2020\nj : |j\u27e9 \u2192 1\u221a\nN\nPN\nk=1|k\u27e9 and identity In. By Lemma 47 in Ref. [12],\n(HnU\u2020\nj )\u2020In = UjHn is a (1 , 0, 0)-encoding of a gram matrix, whose j-th row is 1\u221a\nN (1, . . . ,1). In other\nwords, \u27e8j|UjHn|k\u27e9 = 1\u221a\nN for k \u2208 [N]. By the block encoding multiplication between UjHn and Uf(QKT )",
    "N (1, . . . ,1). In other\nwords, \u27e8j|UjHn|k\u27e9 = 1\u221a\nN for k \u2208 [N]. By the block encoding multiplication between UjHn and Uf(QKT )\nas Lemma 2, we can move the diagonal elements to the j-th row by paying the price of a coefficient 1\u221a\nN .\nThen the constructed unitary is a\n\u0000\u221a\nN, 2n + 3bf + 2, O\n\u0000\u221a\nN((\u03f5s + \u03f5w)\np\nN/Z)\n1\n2\n\u0001\u0001\n-encoding of a matrix \u02dcG,\nwhose j-th row is the same as the j-th row of 1\nZj\nexp \u25e6\n\u0010\nQKT\n\u03b10\n\u0011\n. By Lemma 2 again, one can construct an\n\u0000\n\u03b1v\n\u221a\nN, 2n + 3bf + av + 2, O\n\u0000\u221a\nN((\u03b1v(\u03f5s + \u03f5w)\np\nN/Z)\n1\n2 + \u03f5v)\n\u0001\u0001\n-encoding of the matrix G := \u02dcGV whose j-th\nrow is the same as j-th row of Gsoft := softmax\n\u0010\nQKT\n\u03b10\n\u0011\nV . In total this needs O(Cf \u21132\nq\n1\nZj\n) = O(\u21132/\np\nZj)\ntimes of Us, Uq, Uk and Uv.\nNow we consider how to implement the masked self-attention, which is essential for the decoder-only\nstructure. This can be achieved by slightly changing some steps as introduced in previous theorems.\nCorollary 1 (Quantum masked self-attention) . Consider the same as Problem 1. Let \u03b10 = \u03b12",
    "structure. This can be achieved by slightly changing some steps as introduced in previous theorems.\nCorollary 1 (Quantum masked self-attention) . Consider the same as Problem 1. Let \u03b10 = \u03b12\ns\u03b12\nw. For\nthe index j \u2208 [N], one can construct an\n\u0000\n\u03b1s\u03b1w\n\u221aj, O(\u2113(n + as + aw)), O\n\u0000\u221aj \u00b7\n4\nq\nj\nZj\n\u221a\u03f5s + \u03f5w\n\u0001\u0001\n-encoding of\na matrix Gmask such that Gmask\nj\u22c6 = (softmax( QKT\n\u03b10\n+ M)V )j\u22c6, by using O( \u21132\n\u221a\nZj\n) times of US, UWq , UWk and\nUWv , where M is the masked matrix as Eq. (4), Zj = PN\nk=1 exp \u25e6(QKT\n\u03b10\n+ M)jk, and \u2113 = O(n log( 1\n\u03f5s+\u03f5w\n)).\nProof. To achieve the masked self-attention, we change two places of the previous proof in Theorem 7 and\nTheorem 8. First, in the proof of Theorem 7, we add one more step after constructing a block-encoding of\nmatrix exp \u25e6( A\n2\u03b1 ). For the index j \u2208 [N], we multiply exp \u25e6( A\n2\u03b1 ) with a projector P\nk:k\u2264j|k\u27e9\u27e8k| to mask the\nelements. Though the projector P\nk\u2208S|k\u27e9\u27e8k| for S \u2286 [N] is not unitary in general, one can construct a block",
    "2\u03b1 ) with a projector P\nk:k\u2264j|k\u27e9\u27e8k| to mask the\nelements. Though the projector P\nk\u2208S|k\u27e9\u27e8k| for S \u2286 [N] is not unitary in general, one can construct a block\nencoding of the projector by noticing that it can be written by the linear combination of two unitaries:\nX\nk\u2208S\n|k\u27e9\u27e8k| = 1\n2I + 1\n2\n\u0010\n2\nX\nk\u2208S\n|k\u27e9\u27e8k| \u2212I\n\u0011\n. (26)\nDefine Uproj := |0\u27e9\u27e80| \u2297I + |1\u27e9\u27e81| \u2297(2 P\nk\u2208S |k\u27e9\u27e8k| \u2212I). One can easily verify that ( H \u2297 I)Uproj(H \u2297 I)\nis a (1 , 1, 0)-encoding of P\nk\u2208S |k\u27e9\u27e8k|, where H is the Hadamard gate. Let S = [j], by Lemma 2, one can\nconstruct a ( Cf , bf + 1, \u03b3f )-encoding of exp \u25e6( A\n2\u03b1 ) P\nk:k\u2264j|k\u27e9\u27e8k|, i.e., only add an ancilla qubit in the final\nresult. Note that Zj = PN\nk=1 exp \u25e6(QKT /\u03b1qk + M)jk = Pj\nk=1 exp \u25e6\n\u0000\nQKT /\u03b10\n\u0001\njk.\nSecond, we change the projection operator prepared in Theorem 8. As there are at most j non-zero\nelements in the j-th row for masked self-attention case, it suffices to prepare the isometry Pj\nk=1|k\u27e9\u27e8k|\ninstead of PN\nk=1|k\u27e9\u27e8k|. As a result, the coefficient changes from 1\u221a",
    "elements in the j-th row for masked self-attention case, it suffices to prepare the isometry Pj\nk=1|k\u27e9\u27e8k|\ninstead of PN\nk=1|k\u27e9\u27e8k|. As a result, the coefficient changes from 1\u221a\nN to 1\u221aj .\nOne may further achieve the multi-head self-attention case by using the linear combination of unitaries.\nWe do not describe further details on multi-head attention in this work. For simplicity, in the following,\nwe will directly say we have a ( \u03b1g, ag, \u03f5g)-encoding of G, e.g., \u03b1g = \u03b1s\u03b1w\n\u221a\nN, ag = O(\u2113(n + as + aw)) and\n\u03f5g = O\n\u0010\u221a\nN 4\nq\nN\nZj\n\u221a\u03f5s + \u03f5w\n\u0011\n.\nD. Quantum residual connection and layer normalization\nHere, we first show how to achieve the task as Problem 4 basically following the nonlinear amplitude\ntransformation method [30, 34]. Then, we discuss how to implement the residual connection with layer\nnormalization as Problem 2.\n14In the following, we explicitly consider the residual connection with layer normalization in the transformer\nframework.",
    "normalization as Problem 2.\n14In the following, we explicitly consider the residual connection with layer normalization in the transformer\nframework.\nTheorem 9 (Quantum residual connection with layer normalization) . Consider the setting of Problem 2.\nOne is able to construct an (O(\n\u221a\nd(\u03b1g + \u03b1s)/\u03c2), 2ag + n + 4, O((\u03f5g + \u03f5s)/\u03c2))-state-encoding of the state\ndX\nk=1\nLN(Gsoft\nj , Sj)k|k\u27e9 = 1\n\u03c2\ndX\nk=1\n(Gsoft\njk + Sjk \u2212 \u00afsj)|k\u27e9,\nwhere \u00afsj := 1\nd\nPd\nk=1(Gsoft\njk + Sjk) and \u03c2 :=\nqPd\nk=1(Gsoft\njk + Sjk \u2212 \u00afsj)2.\nProof. As shown in Theorem 8, we can construct an ( \u03b1g, ag, \u03f5g)-encoding of a matrix whose j-th row is\nthe same row as that of Gsoft. By assumption, we are given Us which is an ( \u03b1s, as, \u03f5s)-encoding of S. By\nLemma 1 with state preparation pair ( P, P) such that\nP|0\u27e9 = 1\u221a\u03b1g + \u03b1s\n(\u221a\u03b1g|0\u27e9 + \u221a\u03b1s|1\u27e9), (27)\none can construct a quantum circuit Ures which is an (\u03b1g + \u03b1s, ag + 1, \u03f5g + \u03f5s)-encoding of an N \u00d7 d matrix\nwhose j-th row is the same as that of Gsoft + S.",
    "P|0\u27e9 = 1\u221a\u03b1g + \u03b1s\n(\u221a\u03b1g|0\u27e9 + \u221a\u03b1s|1\u27e9), (27)\none can construct a quantum circuit Ures which is an (\u03b1g + \u03b1s, ag + 1, \u03f5g + \u03f5s)-encoding of an N \u00d7 d matrix\nwhose j-th row is the same as that of Gsoft + S.\nNow we consider how to create a block encoding of a diagonal matrix \u00afsj \u00b7I, where \u00afsj := 1\nd\nPd\nk=1(Gsoft\njk +Sjk).\nLet us define a unitary Hlog d := H\u2297log d. Note that Hlog d is a (1 , 0, 0)-encoding of itself, and the first\ncolumn of Hlog d is 1\u221a\nd (1, . . . ,1)T . By Lemma 2, one can multiply Gsoft + S with Hlog d to construct an\n(\u03b1g + \u03b1s, ag + 1, \u03f5g + \u03f5s)-encoding of an N \u00d7 d matrix, whose (i, 1)-element is\n\u221a\nd\u00afsi. One can further move\nthis element to (1 , 1) by switching the first row with the i-th row. By tensoring with the identity I of log d\nqubits, one can construct an ( \u03b1g + \u03b1s, ag + n + 1, \u03f5g + \u03f5s)-encoding of\n\u221a\nd\u00afsi \u00b7 I.\nWith Uj : |0\u27e9 \u2192 |j\u27e9, one can prepare the state\nU\u2020\nres(I \u2297 Uj)|0\u27e9|0\u27e9 = 1\n\u03b1g + \u03b1s\n|0\u27e9\ndX\nk=1\n\u03c8\u2032\nk|k\u27e9 +\ns\n1 \u2212\nP\nk \u03c8\u20322\nk\n(\u03b1g + \u03b1s)2 |1\u27e9|bad\u27e9, (28)\nwhere |\u03c8\u2032",
    "\u221a\nd\u00afsi \u00b7 I.\nWith Uj : |0\u27e9 \u2192 |j\u27e9, one can prepare the state\nU\u2020\nres(I \u2297 Uj)|0\u27e9|0\u27e9 = 1\n\u03b1g + \u03b1s\n|0\u27e9\ndX\nk=1\n\u03c8\u2032\nk|k\u27e9 +\ns\n1 \u2212\nP\nk \u03c8\u20322\nk\n(\u03b1g + \u03b1s)2 |1\u27e9|bad\u27e9, (28)\nwhere |\u03c8\u2032\nk \u2212 (Gsoft\njk + Sjk)| \u2264\u03f5g + \u03f5s for k \u2208 [d]. By Theorem 6, this can be converted to an ( \u03b1g + \u03b1s, 2ag +\nn + 3, \u03f5g + \u03f5s)-encoding of the diagonal matrix diag( Gj1 + Sj1, . . . , Gjd + Sjd).\nBy Lemma 1 with state preparation pair ( P1, P2), where\nP1|0\u27e9 = 1q\n1 + 1/\n\u221a\nd\n(|0\u27e9 + 1\u221a\nd\n|1\u27e9) (29)\nand\nP2|0\u27e9 = 1q\n1 + 1/\n\u221a\nd\n(|0\u27e9 \u22121\u221a\nd\n|1\u27e9), (30)\none can construct an (( \u03b1g + \u03b1s)(1 + 1/\n\u221a\nd), 2ag + n + 4, (\u03f5g + \u03f5s)(1 + 1/\n\u221a\nd))-encoding of diag( Gj1 + Sj1 \u2212\n\u00afsj, . . . , Gjd + Sjd \u2212 \u00afsj).\nLet this unitary be ULN. Then the unitary ULN(I \u2297 Hlog d) is an (O(\n\u221a\nd(\u03b1g + \u03b1s)/\u03c2), 2ag + n + 4, O((\u03f5g +\n\u03f5s)/\u03c2))-state-encoding of the state\n1\n\u03c2\ndX\nk=1\n(Gsoft\njk + Sjk \u2212 \u00afsj)|k\u27e9,\nwhere \u03c2 :=\nqPd\nk=1(Gsoft\njk + Sjk \u2212 \u00afsj)2.\n15E. Quantum feedforward network",
    "\u221a\nd(\u03b1g + \u03b1s)/\u03c2), 2ag + n + 4, O((\u03f5g +\n\u03f5s)/\u03c2))-state-encoding of the state\n1\n\u03c2\ndX\nk=1\n(Gsoft\njk + Sjk \u2212 \u00afsj)|k\u27e9,\nwhere \u03c2 :=\nqPd\nk=1(Gsoft\njk + Sjk \u2212 \u00afsj)2.\n15E. Quantum feedforward network\nWe turn our attention to the third main building block of the transformer architecture, the feed-forward\nneural network. This block often is a relatively shallow neural network with linear transformations and\nReLU activation functions [3]. More recently, activation functions such as the GELU have become popular,\nbeing continuously differentiable. We highlight that they are ideal for quantum Transformers, since the\nQSVT framework requires functions that are well approximated by polynomial functions. Functions like\nReLU(x) = max(0, x) can not be efficiently approximated. The GELU is constructed from the error function,\nwhich is efficiently approximated as follows.\nLemma 5 (Polynomial approximation of error function [36]) . Let \u03f5 > 0. For every k > 0, the error\nfunction erf(kx) := 2\u221a\u03c0\nRkx\n0 e\u2212t2",
    "which is efficiently approximated as follows.\nLemma 5 (Polynomial approximation of error function [36]) . Let \u03f5 > 0. For every k > 0, the error\nfunction erf(kx) := 2\u221a\u03c0\nRkx\n0 e\u2212t2\ndt can be approximated with error up to \u03f5 by a polynomial function with\ndegree O(k log(1\n\u03f5 )).\nThis lemma, implies the following efficient approximation of the GELU function with polynomials.\nCorollary 2 (Polynomial approximation of GELU function). Let \u03f5 >0 and \u03bb \u2208 O(1). For every k >0 and\nx \u2208 [\u2212\u03bb, \u03bb], the GELU function GELU(kx) := kx \u00b7 1\n2 (1 + erf(kx\u221a\n2 )) can be approximated with error up to \u03f5 by\na polynomial function with degree O(k log(k\u03bb\n\u03f5 )).\nProof. It suffices to approximate the error function with precision \u03f5\nk\u03bb by Lemma 5.\nIn the following theorem, we consider how to implement the two-layer feedforward network. As mentioned,\nthe GELU function is widely used in transformer-based models and we explicitly consider it as the activation",
    "the GELU function is widely used in transformer-based models and we explicitly consider it as the activation\nfunction in the theorem. Cases for other activation functions like sigmoid follow the same analysis. An\nexample is the tanh( x) function, which can be well approximated by a polynomial for x \u2208 [\u2212\u03c0/2, \u03c0/2] [34].\nTheorem 10 (Two-layer feedforward network with GELU function). Consider the setting as in Problem 3.\nLet the activation function be GELU(x) := x \u00b7 1\n2 (1 + erf( x\u221a\n2 )). One can prepare an (O(\u03b1\u03b12\nm/C), 2a + n +\n2am + 4, O((\n\u221aN2\nC \u03b12\nm\u2113\u2032\u221a\u03b1m\u03f5 + \u03f5m)\n1\n2 ))-state-encoding of the state\n|\u03d5\u27e9 = 1\nC\nN2X\nk=1\n\u0010\nM2 \u00b7 GELU(M1 \u00b7 \u03c8)\n\u0011\nk\n|k\u27e9, (31)\nby using \u2113\u2032 times of U\u03c8 and U\u2020\n\u03c8, where C is the normalization factor and \u2113\u2032 = \u02dcO(\u03b1\u03b1m log(1/\u03f5m)).\nProof. Let the erroneous block-encoded matrices be M\u2032\n1 and M\u2032\n2. We have\n(Ia \u2297 UM1 )(Iam \u2297 U\u03c8)|0a+am+n\u27e9 = 1\n\u03b1\u03b1m\n|0a+am \u27e9M\u2032\n1|\u03c8\u2032\u27e9 + |e\u22a5\u27e9, (32)\nwhere |e\u22a5\u27e9 is an unnormalized orthogonal state. For the case N1 \u2265 N, this can be achieved by padding",
    "1 and M\u2032\n2. We have\n(Ia \u2297 UM1 )(Iam \u2297 U\u03c8)|0a+am+n\u27e9 = 1\n\u03b1\u03b1m\n|0a+am \u27e9M\u2032\n1|\u03c8\u2032\u27e9 + |e\u22a5\u27e9, (32)\nwhere |e\u22a5\u27e9 is an unnormalized orthogonal state. For the case N1 \u2265 N, this can be achieved by padding\nancilla qubits to the initial state. By direct computation, we have\n\u2225M1|\u03c8\u27e9 \u2212M\u2032\n1|\u03c8\u2032\u27e9\u2225\u221e\n\u2264\u2225M1|\u03c8\u27e9 \u2212M1|\u03c8\u2032\u27e9 + M1|\u03c8\u2032\u27e9 \u2212M\u2032\n1|\u03c8\u2032\u27e9\u2225\u221e\n\u2264\u2225M1|\u03c8\u27e9 \u2212M1|\u03c8\u2032\u27e9\u2225\u221e + \u2225M1|\u03c8\u2032\u27e9 \u2212M\u2032\n1|\u03c8\u2032\u27e9\u2225\u221e\n\u2264\u2225M1\u2225\u2225|\u03c8\u27e9 \u2212 |\u03c8\u2032\u27e9\u2225\u221e + \u2225M1 \u2212 M\u2032\n1\u2225\u2225|\u03c8\u2032\u27e9\u2225\u221e\n\u2264\u03b1m\u03f5 + \u03f5m. (33)\nBy Theorem 6, one can construct an (\u03b1\u03b1m, a+n+2, \u03b1m\u03f5+\u03f5m)-encoding of matrix diag((M1\u03c8)1, . . . ,(M1\u03c8)N1 ).\nNote that the GELU function does not have a constant term, and is suitable to use the importance-weighted\namplitude transformation as in Ref. [30]. Instead of directly implementing the GELU function, we first\n16implement the function f(x) = 1\n2 (1 + erf( x\u221a\n2 )). Note that the value of |erf(x)| is upper bounded by 1. By\nTheorem 6 with function 1\n4 (1+erf( \u03b1\u03b1m x\u221a\n2 )), one can construct a (2, a+n+4, 4\u2113\u221a\u03b1m\u03f5 + \u03f5m +\u03b3+\u03b4)-encoding",
    "2 (1 + erf( x\u221a\n2 )). Note that the value of |erf(x)| is upper bounded by 1. By\nTheorem 6 with function 1\n4 (1+erf( \u03b1\u03b1m x\u221a\n2 )), one can construct a (2, a+n+4, 4\u2113\u221a\u03b1m\u03f5 + \u03f5m +\u03b3+\u03b4)-encoding\nof matrix diag(f(M1\u03c8)1, . . . , f(M1\u03c8)N1 ), where \u2113 = \u02dcO(\u03b1\u03b1m log(1/\u03b3)).\nLet the previously constructed block-encoding unitary be Uf(x). We have\nUf(x)(I \u2297 UM1 )(I \u2297 U\u03c8)|0\u27e9|0\u27e9 = 1\n2\u03b1\u03b1m\n|0\u27e9\nX\nk\nGELU\u2032(M\u2032\n1\u03c8\u2032)k|k\u27e9 + |f\u22a5\u2032\u27e9, (34)\nwhere |f\u22a5\u2032\u27e9 is an unnormalized orthogonal state. Setting \u03b3, \u03b4= O(\u03f5m), by direct computation, we have\n\u2225GELU\u2032(M\u2032\n1\u03c8\u2032) \u2212 GELU(M1\u03c8)\u2225\u221e\n=\u2225M\u2032\n1\u03c8\u2032f\u2032(M\u2032\n1\u03c8\u2032) \u2212 M1\u03c8f(M1\u03c8)\u2225\u221e\n\u2264\u2225M\u2032\n1\u03c8\u2032f\u2032(M\u2032\n1\u03c8\u2032) \u2212 M\u2032\n1\u03c8\u2032f(M1\u03c8)\u2225\u221e + \u2225M\u2032\n1\u03c8\u2032f(M1\u03c8) \u2212 M1\u03c8f(M1\u03c8)\u2225\u221e\n\u2264\u03b1m(4\u2113\u221a\u03b1m\u03f5 + \u03f5m + \u03b3 + \u03b4) + \u03b1m\u03f5 + \u03f5m = O(\u03b1m\u2113\u221a\u03b1m\u03f5 + \u03f5m). (35)\nFinally, by implementing the block-encoding unitary UM2 , we have\n(I \u2297 UM2 )(I \u2297 Uf(x))(I \u2297 UM1 )(I \u2297 U\u03c8)|0\u27e9|0\u27e9\n= C\u2032\n2\u03b1\u03b12m\n|0\u27e9 1\nC\u2032\nX\nj\n\u03c8fin|j\u27e9 + |f\u22a5\u2032\u2032\u27e9, (36)\nwhere C\u2032 is the exact normalization factor, \u2225\u03c8inf \u2212 M2GELU(M1\u03c8)\u2225\u221e = O(\u03b12\nm\u2113\u2032\u221a\u03b1m\u03f5 + \u03f5m + \u03f5m) =\nO(\u03b12\nm\u2113\u2032\u221a\u03b1m\u03f5 + \u03f5m), and |e\u22a5\n\u2032\u2032",
    "= C\u2032\n2\u03b1\u03b12m\n|0\u27e9 1\nC\u2032\nX\nj\n\u03c8fin|j\u27e9 + |f\u22a5\u2032\u2032\u27e9, (36)\nwhere C\u2032 is the exact normalization factor, \u2225\u03c8inf \u2212 M2GELU(M1\u03c8)\u2225\u221e = O(\u03b12\nm\u2113\u2032\u221a\u03b1m\u03f5 + \u03f5m + \u03f5m) =\nO(\u03b12\nm\u2113\u2032\u221a\u03b1m\u03f5 + \u03f5m), and |e\u22a5\n\u2032\u2032\n\u27e9 is an unnormalized orthogonal state. By Lemma 4, we have\n\r\r\r 1\nC\u2032 \u03c8inf \u2212 1\nC M2GELU(M1\u03c8)\n\r\r\r\n\u221e\n= O\n \u0012\u221aN2\nC \u03b12\nm\u2113\u2032\u221a\u03b1m\u03f5 + \u03f5m\n\u00131\n2\n!\n. (37)\nF. Quantum single-layer transformer\nCombining the previous results, one can obtain the following result. Note that for a single-layer trans-\nformer, we mean the same as Fig. 1, i.e., combined with a self-attention block, a two-layer feedforward\nnetwork, and two residual connection with layer normalization blocks. The proof is provided in Appendix G.\nTheorem 11 (Quantum single-layer Transformer) . Let the input assumptions be as in Definition 4. If\n\u03f5s, \u03f5w, \u03f5m = O(\u03f58\u03b1\u221226\nm d\u22124\u03c22\u03c2\u20328\nq\nZj\nN\n1\nN ), then for the index j \u2208 [N], one can construct a (1, O(\u2113(n + as +\naw) + aM ), \u03f5)-state-encoding of a quantum state proportional to\ndX\nk=1\nTransformer(S, j)k|k\u27e9, (38)\nby using O(d\u03b1s\u03b1w\u03b13\nm\u21132\nq",
    "N\n1\nN ), then for the index j \u2208 [N], one can construct a (1, O(\u2113(n + as +\naw) + aM ), \u03f5)-state-encoding of a quantum state proportional to\ndX\nk=1\nTransformer(S, j)k|k\u27e9, (38)\nby using O(d\u03b1s\u03b1w\u03b13\nm\u21132\nq\nN\nZj\n1\n\u03c2\u03c2\u2032 log( 1\n\u03f5m\n)) times of US, UWq , UWk , UWv and UM , where \u2113 = O(n log( 1\n\u03f5s+\u03f5w\n)),\nZj = PN\nk=1 exp \u25e6(QKT /\u03b12\ns\u03b12\nw)jk, and \u03c2, \u03c2\u2032 are standard deviations from two layer normalization blocks.\nOne can arrive the informal version (Theorem 1) by assuming \u03b1m = O(1), Zj = \u2126(N), and \u03c2, \u03c2\u2032 = \u2126(1).\nTo obtain the classical output, one can perform the quantum state tomography. Here, we use the \u2113\u221e-norm\ntomography for the analysis. Note that we change from the time complexity to the query complexity to\nmatch the analysis in this paper.\n17Theorem 12 (\u2113\u221e state tomography [37]) . Given access to a quantum circuit U : |0\u27e9 \u2192 |\u03c8\u27e9, there is a\ntomography algorithm that produces unit vector \u03c8\u2032 \u2208 Rd such that \u2225\u03c8\u2032 \u2212 \u03c8\u2225\u221e \u2264 \u03b4 with probability at least",
    "tomography algorithm that produces unit vector \u03c8\u2032 \u2208 Rd such that \u2225\u03c8\u2032 \u2212 \u03c8\u2225\u221e \u2264 \u03b4 with probability at least\n1 \u2212 1/poly(d) by using O(log d/\u03b42) times of controlled-U.\nBy setting \u03b4 = O(\u03f5), one can read out all the values Transformer( S, j)k with precision O(\u03f5) for k \u2208 [d].\nOne can implement the process for all focused tokens j \u2208 [N] to obtain the information required by the next\nlayer\u2019s self-attention block.\nV. NUMERICAL STUDIES OF QUANTUM-RELEVANT PROPERTIES OF REAL-WORLD\nLLMS\nIn this section, we provide numerical investigations of popular open-source LLMs in terms of their connec-\ntion to our quantum implementation of the transformer. In particular, we focus on the key quantities that\ndetermine the run time of the quantum transformer, which arise from the given input. There are multiple\nways to construct the block encoding as given in the input assumption Definition 4, which we describe in",
    "ways to construct the block encoding as given in the input assumption Definition 4, which we describe in\nIf it is possible to have access to the qRAM and quantum data structure, one can construct a block encoding\nfor an arbitrary matrix, paying the price that the normalization factor will be the Frobenius norm of block\nencoded matrix. Appendix A. Based on this consideration and to obtain a better intuition, we numerically\n100 101 102\n101\n102\nFrobenius Norm\n100 101 102\n101\n102\n100 101 102\n100\n100 101 102\nSentence Length N\n101\n102\n103\nSpectral Norm\n100 101 102\nSentence Length N\n101\n102\n100 101 102\nSentence Length N\n100\n101\ny x\n bert roberta distilgpt gpt2 gpt llama2-7b tinyllama mistral7b\nFIG. 2. Scaling of the spectral norm \u2225S\u2225 and the Frobenius norm \u2225S\u2225F with N for each model, displayed on\nlogarithmic scales for both axes. For reference, the line y \u221d \u221ax is also shown. We randomly generate tokens and\nconvert them to S.",
    "logarithmic scales for both axes. For reference, the line y \u221d \u221ax is also shown. We randomly generate tokens and\nconvert them to S.\nstudy several open-source large language models 2. We first investigate the spectral and Frobenius norm of\n2 Parameters are obtained from the Hugging Face website, which is an open-source platform for machine learning models.\n18the input sequence matrix S. To demonstrate how the norms of S scale with the length N, we randomly\nsample tokens from the tokenizer that each pretrained model uses and then perform inference on the model\nwith the generated dataset. The results are shown in Fig. 2. The norms seen in Fig. 2 are calculated by\nsumming the input embedding with the positional embedding, and lastly computing the respective norms\non the resulting vector. We observe that the spectral norm scales almost sublinearly with O(\n\u221a\nN) and the\nFrobenius norm scales as O(\n\u221a\nN).",
    "on the resulting vector. We observe that the spectral norm scales almost sublinearly with O(\n\u221a\nN) and the\nFrobenius norm scales as O(\n\u221a\nN).\nWe also consider data in real-world applications, such as samples from the widely-used Massive Multitask\nLanguage Understanding (MMLU) dataset [38] covering 57 subjects across STEM, the humanities, the\nsocial sciences, and more. The scaling of spectral norm and Frobenius norm of S on the MMLU dataset is\ndemonstrated in Fig. 3. Again, the results of the DistilGPT almost overlap with those of GPT2. We see that\nin some of the models, the variances of the Frobenius norm and/or the spectral norm for a given N are large\ncompared to those of the random dataset. The large variances are arguably the consequence of the training\nin those models; the embeddings that frequently appear in the real-world dataset are actively updated at\nthe pre-training stage, and therefore, are more broadly distributed as a result of the pre-training. In models",
    "the pre-training stage, and therefore, are more broadly distributed as a result of the pre-training. In models\nwith relatively small variance, e.g., BERT, GPT, and Llama2-7b, the spectral norm and the Frobenius norm\nsublinearly scale as O(\n\u221a\nN). It is notable that the spectral norms in BERT and Roberta even decrease with\nthe value of N. This can be caused by the correlations between the embeddings; the embeddings appear in\nthe longer sentences may be correlated with each other in those models, resulting in a smaller spectral norm.\n101 102\n102\nFrobenius Norm\n101 102\n101\n102\n101 102 103\n100\n101\n101 102\nSentence Length N\n102\n103\nSpectral Norm\n101 102\nSentence Length N\n102\n101 102 103\nSentence Length N\n100\n101\ny x\n bert roberta distilgpt gpt2 gpt llama2-7b tinyllama mistral7b\nFIG. 3. Scaling of the spectral norm \u2225S\u2225 and the Frobenius norm \u2225S\u2225F with N for each model, displayed on",
    "Sentence Length N\n100\n101\ny x\n bert roberta distilgpt gpt2 gpt llama2-7b tinyllama mistral7b\nFIG. 3. Scaling of the spectral norm \u2225S\u2225 and the Frobenius norm \u2225S\u2225F with N for each model, displayed on\nlogarithmic scales for both axes. For reference, the line y \u221d \u221ax is also shown. We use tokens in MMLU dataset and\nconvert them to S.\nWe then compute the spectral and Frobenius norms of weight matrices (Wq, Wk, Wv) for the large language\nmodels. The result can be seen in Fig. 4. Many of the LLMs below a dimension d of 103 that we have checked\nhave substantially different norms. We observe that for larger models such as Llama2-7b and Mistral-7b,\nwhich are also current state-of-the-art open-source models, the norms do not change dramatically. Based on\nthese, our assumption is that the spectral norm and the Frobenius norm of the weight matrices as a function\n19of d scale roughly O(polylog(d)) for advanced LLMs.\n1000 2000 3000 4000\n25\n50\n75\n100\n125\n150\n175Frobenius Norm\n1000 2000 3000 4000\n0\n25",
    "19of d scale roughly O(polylog(d)) for advanced LLMs.\n1000 2000 3000 4000\n25\n50\n75\n100\n125\n150\n175Frobenius Norm\n1000 2000 3000 4000\n0\n25\n50\n75\n100\n125\n150\n175\nbert roberta distilgpt gpt2 gpt1 llama2-7b tinyllama mistral-7b\n1000 2000 3000 4000\n10\n20\n30\n40\n1000 2000 3000 4000\nDimension of the weight matrix Wq\n5\n10\n15\n20\n25\n30Spectral Norm\n1000 2000 3000 4000\nDimension of the weight matrix Wk\n10\n20\n30\n40\n1000 2000 3000 4000\nDimension of the weight matrix Wv\n0\n2\n4\n6\n8\n10\nFIG. 4. Norms of weight matrices in popular open-source LLMs. We compute the spectral and Frobenius norms of\nthe weight matrices Wq, Wk, and Wv in the first layer. Note that for the multi-head self-attention, matrices have\nbeen concatenated to achieve the square matrix.\nUnder the label of efficient transformer [39], many classical works utilize ideas like sparsification and low\nrank approximation to make the matrix computation more efficient. These results may also benefit the",
    "rank approximation to make the matrix computation more efficient. These results may also benefit the\nquantum side, e.g., being able to use the standard sparse oracle for block encoding. Other methods may also\nbe possible to achieve the input assumption based on more understanding of the input sequence and weight\nmatrices.\nVI. EXTENSIONS\nWe briefly describe several extensions of our work such as a multi-layer architecture, next token outputs,\nand steps toward a trainable architecture.\nGeneralization to multi-layer architectures with tomography \u2014 We describe briefly how to generalize to\nthe multi-layer transformer. As shown in Theorem 11, we can construct a quantum state containing the\ninformation of a single-layer transformer. By using quantum state tomography as described in Theorem 12,\none can obtain all the information classically. To move to the next layer, we need a way to encode the",
    "one can obtain all the information classically. To move to the next layer, we need a way to encode the\nclassical information. With the assumption of qRAM and quantum data structure, one constructs the block\nencoding of the input sequence matrix for the next layer. While the dependency on the error after multiple\nlayers will accumulate geometrically, there is evidence from classical AI research [24] that LLMs are quite\nrobust.\nClassical output \u2014 Now we discuss how to generate the same output as the classical transformer. Clas-\nsically, the next predicted token is obtained by first mapping the vector to dimension dtoken, the number\nof different tokens, via a linear transformation, then implementing a softmax function and sample from the\n20distribution. Note that dtoken is comparatively small to N. The first way is performing the tomography,\nand implementing the output block classically. One may also consider implementing the output block on the",
    "and implementing the output block classically. One may also consider implementing the output block on the\nquantum computer, which can be achieved with the method introduced in this paper. However, this method\nis only suitable for the single-layer case, and how to generalize to multiple layers without measurement\nremains an open problem.\nTrainable architecture \u2014 For the trainability of the architecture, we require trainable parameters and\na loss function. So far, we have assumed that the weights are pre-trained and made available via block-\nencodings. The modularity of the block-encoding framework allows to swap the assumed block encodings\nfor parameterized block encodings, that contain trainable parameters. We provide a formal definition for a\ntrainable block encoding here and note that the definition contains the usual variational circuits and allows\nfor more general circuits.\nDefinition 5 (Parameterized block encoding (PBE)) . Let \u03b8 \u2208 R M where M is the number of parameters,",
    "for more general circuits.\nDefinition 5 (Parameterized block encoding (PBE)) . Let \u03b8 \u2208 R M where M is the number of parameters,\nA(\u03b8) \u2208 C 2n\u00d72n\nand \u03b1(\u03b8) > 0 such that \u2225A(\u03b8)\u2225/\u03b1(\u03b8) \u2264 1. We say a unitary U is a (\u03b1(\u03b8), a, \u03f5) parameterized\nblock encoding if U is a (\u03b1(\u03b8), a, \u03f5) block encoding of A(\u03b8).\nFor training, the main strategy is to use the loss functions from the classical architectures [3] and results\nfrom tomography [37, 40]. While we expect that issues such as barren plateaus [41, 42] will appear, especially\nfor variational PBEs, there could be room for efficient training arising from the discussed possible quantum\nadvantages of the inference step. We leave a discussion of PBEs and transformer architecture training for\nfuture work. It would also be interesting to consider a comparison of the more general definition of PBEs\nand variational circuits in light of the barren plateau issue.\nVII. DISCUSSION",
    "future work. It would also be interesting to consider a comparison of the more general definition of PBEs\nand variational circuits in light of the barren plateau issue.\nVII. DISCUSSION\nWe show in this work progress towards implementing transformer architectures on fault-tolerant quantum\ncomputers. We show how to formulate and achieve each block of the transformer as quantum subroutines,\nwhich can be further combined in a modular fashion. We have discussed the relevant input quantities for\nour quantum subroutines and their behavior in real-world large-language models. We discuss here several\nfurther aspects such as possible quantum advantages, and open directions.\nRelated works \u2014 We note previous works on quantum algorithms for (part of) the transformer architecture\n[43\u201346]. They consider the quantum analog version in the variational quantum circuit setting or focus on\nthe self-attention matrix computation based on the Grover algorithm. After the first preprint version of this",
    "the self-attention matrix computation based on the Grover algorithm. After the first preprint version of this\nwork, another work on a quantum algorithm for the transformer appeared [47].\nPossible quantum advantage \u2014 The ability to obtain a quantum advantage hinges on how the input is\ngiven and the particular problem. We do not provide a provable end-to-end advantage here, but rather\ndevelop the pertinent quantum subroutines and combine them into a transformer architecture. Given the\ninput, our subroutines are efficient in several aspects. They use a number of working plus ancilla qubits that\nis logarithmic in the problem size specified by the sequence length N and the embedding size d. The use of\namplification and its cost depends on the final task at hand. A regime for a possible quantum advantage is\nsummarized in the Table II. According to our numerical observations on the spectral norm and Frobenius",
    "summarized in the Table II. According to our numerical observations on the spectral norm and Frobenius\nnorm of matrices S, Wq, Wk, and Wv, the regime for the normalization factors in the table is reasonable\nand can be broader in possible real-world scenarios. Based on these assumption, we obtain a number of\nqueries to the input of eO(d\n3\n2\n\u221a\nN). The classical run time is O(Nd + d2). We note that the efficiency of the\nsubroutines allows for the potential for larger speedups in other regimes.\nFuture research directions and open problems \u2014We conclude with several remaining questions and research\ndirections. First, we leave open the detailed discussion about the complexities of multilayer architectures. In\nmany cases with naive concatenation of our subroutines, the complexity will be exponential in the number\nof layers. Are there situations where this dependence on the number of layers can be avoided?",
    "many cases with naive concatenation of our subroutines, the complexity will be exponential in the number\nof layers. Are there situations where this dependence on the number of layers can be avoided?\nSecond, we leave open a more detailed analysis of the required quantum resources. Our asymptotic analysis\ndoes not explicitly specify multiplicative constants in the complexity. Concrete problem settings with specific\nparameters for the input should be investigated to see if there exists the possibility for an end-to-end quantum\nadvantage.\n21Quantity Symbol Regime\nSoftmax normalization factor Zj \u2126(N)\nSequence matrix normalization \u03b1s O(\n\u221a\nN)\nAttention weight matrix normalization \u03b1w O(\n\u221a\nd)\nLayer normalization factors \u03c2, \u03c2\u2032 \u2126(1)\nFNN matrix normalization \u03b1m O(1)\nFinal output error \u03f5 \u2126(1/N)\nTABLE II. A possible regime for the transformer where a quantum advantage could be exhibited, based on our result\nin Theorem 11.",
    "FNN matrix normalization \u03b1m O(1)\nFinal output error \u03f5 \u2126(1/N)\nTABLE II. A possible regime for the transformer where a quantum advantage could be exhibited, based on our result\nin Theorem 11.\nIn addition, we have not considered the training of the transformer architectures in great detail. The\nweight matrices are determined from a large data set of training data and the optimization of a loss func-\ntion. Embedding large data into quantum computers is difficult in the absence of the availability of func-\ntioning quantum RAMs. The iterative update of the weights may incur significant overheads in terms of\nmeasurements. Hence, it could be an interesting direction to train the weights on quantum data, which may\nallow for a more direct construction of the weight-matrix block encodings. It may also be interesting to\nexplore whether the residual connection, described as one key block here, may improve the trainability of",
    "explore whether the residual connection, described as one key block here, may improve the trainability of\nparameterized quantum circuits similar to how it improves the trainability of classical neural networks.\nAcknowledgement This research is supported by the National Research Foundation, Singapore, and\nA*STAR under its CQT Bridging Grant and its Quantum Engineering Programme under grant NRF2021-\nQEP2-02-P05. KN acknowledges the support of Grant-in-Aid for JSPS Research Fellow 22J01501. NG\nthanks Fuzhao Xue and Lizhi Lin for their insightful discussions about classical transformer architectures.\nWe thank Po-Wei Huang for valuable suggestions on the current version.\n[1] OpenAI. GPT-4 technical report. arXiv:2303.08774, 2023.\n[2] S\u00b4 ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,\nYin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang.",
    "Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang.\nSparks of artificial general intelligence: Early experiments with GPT-4. arXiv:2303.12712, 2023.\n[3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,  Lukasz Kaiser, and\nIllia Polosukhin. Attention is All you Need. In Advances in Neural Information Processing Systems , volume 30.\nCurran Associates, Inc., 2017.\n[4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to\nalign and translate. arXiv:1409.0473, 2016.\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv:1810.04805, 2019.\n[6] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understand-\ning by generative pre-training. https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-",
    "ing by generative pre-training. https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-\nunsupervised/language understanding paper.pdf, 2018.\n[7] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Lan-\nguage models are unsupervised multitask learners. https://d4mucfpksywv.cloudfront.net/better-language-\nmodels/language models are unsupervised multitask learners.pdf, 2019.\n[8] Tom B. Brown, Benjamin Mann, Nick Ryder, and et al. Language models are few-shot learners.arXiv:2005.14165,\n2020.\n[9] Aram W. Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum Algorithm for Linear Systems of Equations.\nPhysical Review Letters, 103(15):150502, October 2009.\n[10] Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum principal component analysis. Nature Physics,\n10(9):631\u2013633, September 2014.\n[11] Guang Hao Low and Isaac L. Chuang. Optimal Hamiltonian Simulation by Quantum Signal Processing. Physical\nReview Letters, 118(1):010501, January 2017.",
    "10(9):631\u2013633, September 2014.\n[11] Guang Hao Low and Isaac L. Chuang. Optimal Hamiltonian Simulation by Quantum Signal Processing. Physical\nReview Letters, 118(1):010501, January 2017.\n22[12] Andr\u00b4 as Gily\u00b4 en, Yuan Su, Guang Hao Low, and Nathan Wiebe. Quantum singular value transformation and\nbeyond: Exponential improvements for quantum matrix arithmetics. In Proceedings of the 51st Annual ACM\nSIGACT Symposium on Theory of Computing , pages 193\u2013204, June 2019.\n[13] Guang Hao Low and Yuan Su. Quantum eigenvalue processing. arXiv:2401.06240, 2024.\n[14] Laird Egan, Dripto M. Debroy, Crystal Noel, Andrew Risinger, Daiwei Zhu, Debopriyo Biswas, Michael Newman,\nMuyuan Li, Kenneth R. Brown, Marko Cetina, and Christopher Monroe. Fault-tolerant control of an error-\ncorrected qubit. Nature, 598(7880):281\u2013286, October 2021.\n[15] Google Quantum AI. Suppressing quantum errors by scaling a surface code logical qubit. Nature, 614(7949):676\u2013\n681, February 2023.",
    "corrected qubit. Nature, 598(7880):281\u2013286, October 2021.\n[15] Google Quantum AI. Suppressing quantum errors by scaling a surface code logical qubit. Nature, 614(7949):676\u2013\n681, February 2023.\n[16] Dolev Bluvstein, Simon J. Evered, Alexandra A. Geim, and Et al. Logical quantum processor based on recon-\nfigurable atom arrays. Nature, 626(7997):58\u201365, 2024.\n[17] Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone. Quantum random access memory. Phys. Rev. Lett. ,\n100:160501, Apr 2008.\n[18] Samuel Jaques and Arthur G. Rattew. Qram: A survey and critique. arXiv:2305.10310, 2023.\n[19] Guang Hao Low and Isaac L. Chuang. Hamiltonian Simulation by Qubitization. Quantum, 3:163, July 2019.\n[20] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword\nunits. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th Annual Meeting of the Associa-",
    "units. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers) , pages 1715\u20131725, Berlin, Germany, August 2016.\nAssociation for Computational Linguistics.\n[21] Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and\ndetokenizer for neural text processing. In Eduardo Blanco and Wei Lu, editors,Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing: System Demonstrations , pages 66\u201371, Brussels, Belgium,\nNovember 2018. Association for Computational Linguistics.\n[22] Sabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey, Matthias Gall\u00b4 e, Arun Raja,\nChenglei Si, Wilson Y. Lee, Beno\u02c6 \u0131t Sagot, and Samson Tan. Between words and characters: A brief history of\nopen-vocabulary modeling and tokenization in nlp. arXiv:2112.10508, 2021.",
    "Chenglei Si, Wilson Y. Lee, Beno\u02c6 \u0131t Sagot, and Samson Tan. Between words and characters: A brief history of\nopen-vocabulary modeling and tokenization in nlp. arXiv:2112.10508, 2021.\n[23] Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub\nPachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot\nhyperparameter transfer. arXiv:2203.03466, 2022.\n[24] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang,\nJilong Xue, and Furu Wei. The era of 1-bit llms: All large language models are in 1.58 bits. arXiv:2402.17764,\n2024.\n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\narXiv:1512.03385, 2015.\n[26] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv:1607.06450, 2016.",
    "arXiv:1512.03385, 2015.\n[26] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv:1607.06450, 2016.\n[27] Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error linear\nunits. https://openreview.net/forum?id=Bk0MRI5lg, 2017.\n[28] Shantanav Chakraborty, Andr\u00b4 as Gily\u00b4 en, and Stacey Jeffery. The Power of Block-Encoded Matrix Powers:\nImproved Regression Techniques via Faster Hamiltonian Simulation. In Christel Baier, Ioannis Chatzigiannakis,\nPaola Flocchini, and Stefano Leonardi, editors, 46th International Colloquium on Automata, Languages, and\nProgramming (ICALP 2019) , volume 132 of Leibniz International Proceedings in Informatics (LIPIcs) , pages\n33:1\u201333:14, Dagstuhl, Germany, 2019. Schloss Dagstuhl \u2013 Leibniz-Zentrum f\u00a8 ur Informatik.\n[29] Iordanis Kerenidis and Anupam Prakash. Quantum Recommendation Systems. arXiv:1603.08675, September\n2016.",
    "33:1\u201333:14, Dagstuhl, Germany, 2019. Schloss Dagstuhl \u2013 Leibniz-Zentrum f\u00a8 ur Informatik.\n[29] Iordanis Kerenidis and Anupam Prakash. Quantum Recommendation Systems. arXiv:1603.08675, September\n2016.\n[30] Arthur G. Rattew and Patrick Rebentrost. Non-linear transformations of quantum amplitudes: Exponential\nimprovement, generalization, and applications. arXiv:2309.09839, 2023.\n[31] Gilles Brassard, Peter H\u00f8yer, Michele Mosca, and Alain Tapp. Quantum amplitude amplification and estimation.\nIn Quantum Computation and Information (Washington, DC, 2000) , volume 305 of Contemporary Mathematics,\npages 53\u201374. American Mathematical Society, Providence, RI, 2002.\n[32] Xiaoming Sun, Guojing Tian, Shuai Yang, Pei Yuan, and Shengyu Zhang. Asymptotically Optimal Circuit\nDepth for Quantum State Preparation and General Unitary Synthesis. IEEE Transactions on Computer-Aided\nDesign of Integrated Circuits and Systems , 42(10):3301\u20133314, October 2023.",
    "Depth for Quantum State Preparation and General Unitary Synthesis. IEEE Transactions on Computer-Aided\nDesign of Integrated Circuits and Systems , 42(10):3301\u20133314, October 2023.\n[33] Xiao-Ming Zhang, Tongyang Li, and Xiao Yuan. Quantum State Preparation with Optimal Circuit Depth:\nImplementations and Applications. Physical Review Letters, 129(23):230504, November 2022.\n[34] Naixu Guo, Kosuke Mitarai, and Keisuke Fujii. Nonlinear transformation of complex amplitudes via quantum\nsingular value transformation. arXiv:2107.10764, 2021.\n[35] Liming Zhao, Zhikuan Zhao, Patrick Rebentrost, and Joseph Fitzsimons. Compiling basic linear algebra sub-\nroutines for quantum computers. Quantum Machine Intelligence , 3(2):21, June 2021.\n[36] Guang Hao Low. Quantum Signal Processing by Single-Qubit Dynamics . Thesis, Massachusetts Institute of\n23Technology, 2017.\n[37] Iordanis Kerenidis, Jonas Landman, and Anupam Prakash. Quantum algorithms for deep convolutional neural",
    "23Technology, 2017.\n[37] Iordanis Kerenidis, Jonas Landman, and Anupam Prakash. Quantum algorithms for deep convolutional neural\nnetworks. In International Conference on Learning Representations, 2020.\n[38] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding. Proceedings of the International Conference on Learning\nRepresentations (ICLR), 2021.\n[39] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. arXiv:2009.06732,\n2022.\n[40] Hsin-Yuan Huang, Richard Kueng, and John Preskill. Predicting Many Properties of a Quantum System from\nVery Few Measurements. Nature Physics, 16(10):1050\u20131057, October 2020.\n[41] Jarrod R. McClean, Sergio Boixo, Vadim N. Smelyanskiy, Ryan Babbush, and Hartmut Neven. Barren plateaus\nin quantum neural network training landscapes. Nature Communications, 9(1):4812, 2018.",
    "[41] Jarrod R. McClean, Sergio Boixo, Vadim N. Smelyanskiy, Ryan Babbush, and Hartmut Neven. Barren plateaus\nin quantum neural network training landscapes. Nature Communications, 9(1):4812, 2018.\n[42] Martin Larocca, Supanut Thanasilp, Samson Wang, Kunal Sharma, Jacob Biamonte, Patrick J. Coles, Lukasz\nCincio, Jarrod R. McClean, Zo\u00a8 e Holmes, and M. Cerezo. A review of barren plateaus in variational quantum\ncomputing. arXiv:2405.00781, 2024.\n[43] El Amine Cherrat, Iordanis Kerenidis, Natansh Mathur, Jonas Landman, Martin Strahm, and Yun Yvonna Li.\nQuantum vision transformers. arXiv:2209.08167, 2024.\n[44] Guangxi Li, Xuanqiang Zhao, and Xin Wang. Quantum self-attention neural networks for text classification.\narXiv:2205.05625, 2023.\n[45] Yeqi Gao, Zhao Song, Xin Yang, and Ruizhe Zhang. Fast quantum algorithm for attention computation.\narXiv:2307.08045, 2023.\n[46] Riccardo Di Sipio, Jia-Hong Huang, Samuel Yen-Chi Chen, Stefano Mangini, and Marcel Worring. The dawn",
    "arXiv:2307.08045, 2023.\n[46] Riccardo Di Sipio, Jia-Hong Huang, Samuel Yen-Chi Chen, Stefano Mangini, and Marcel Worring. The dawn\nof quantum natural language processing. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pages 8612\u20138616, 2022.\n[47] Yidong Liao and Chris Ferrie. Gpt on a quantum computer. arXiv:2403.09418, 2024.\n[48] Kosuke Mitarai, Masahiro Kitagawa, and Keisuke Fujii. Quantum analog-digital conversion. Phys. Rev. A ,\n99:012301, Jan 2019.\n24Supplementary Material\nAppendix A: Construction of block encoding unitaries\nIn this section, we summarize some methods to construct a block-encoding unitary. The first method is\napplicable to sparse matrices. As mentioned in [39], there are many works considering the sparsification of\nattention matrices. Quantum may also benefit from these results.\nLemma S1 (Block-encoding of sparse-access matrices [12]) . Let A \u2208 C2n\u00d72n\nbe a matrix that is sr-row-",
    "attention matrices. Quantum may also benefit from these results.\nLemma S1 (Block-encoding of sparse-access matrices [12]) . Let A \u2208 C2n\u00d72n\nbe a matrix that is sr-row-\nsparse and sc-column-sparse, and each element of A has absolute value at most 1. Suppose that we have\naccess to the following sparse-access oracles acting on two (n + 1) qubit registers\nOr : |i\u27e9|k\u27e9 \u2192 |i\u27e9|rik\u27e9 \u2200i \u2208 [2w] \u2212 1, k\u2208 [sr], and\nOc : |\u2113\u27e9|j\u27e9 \u2192 |c\u2113j\u27e9|j\u27e9 \u2200\u2113 \u2208 [sc], j\u2208 [2n] \u2212 1, where\nrij is the index for the j-th non-zero entry of the i-th row of A, or if there are less than i non-zero entries,\nthen it is j + 2n, and similarly cij is the index for the i-th non-zero entry of the j-th column of A, or if there\nare less than j non-zero entries, then it is i + 2n. Additionally assume that we have access to an oracle OA\nthat returns the entries of A in a binary description\nOA : |i\u27e9|j\u27e9|0\u27e9\u2297b \u2192 |i\u27e9|j\u27e9|aij\u27e9 \u2200i, j\u2208 [2w] \u2212 1, where\naij is a b-bit binary description of the Aij. Then we can implement a\n\u0000\u221asrsc, n+ 3, \u03b5\n\u0001",
    "that returns the entries of A in a binary description\nOA : |i\u27e9|j\u27e9|0\u27e9\u2297b \u2192 |i\u27e9|j\u27e9|aij\u27e9 \u2200i, j\u2208 [2w] \u2212 1, where\naij is a b-bit binary description of the Aij. Then we can implement a\n\u0000\u221asrsc, n+ 3, \u03b5\n\u0001\n\u2212 block-encoding of\nA with a single use of Or, Oc, two uses of OA and additionally using O\n\u0000\nn + log2.5\u0000srsc\n\u03b5\n\u0001\u0001\none and two qubit\ngates while using O\n\u0010\nb, log2.5\n\u0010\n(srsc\n\u03b5\n\u0011\u0011\nancilla qubits.\nThe second method is for general matrices, yet we need some further assumptions which may not be easy\nto achieve.\nLemma S2 (Block-encodings of matrices stored in quantum data structures [12, 29]) . Let A \u2208 C2n\u00d72n\n.\nFor q \u2208 [0, 2], let us define \u00b5q(A) =\nq\nwq(A)w(2\u2212q)(AT ), where wq(A) := max i\u2225ai.\u2225q\nq is the q-th power\nof the maximum q-norm of the rows of A. Let A(q) denote the matrix of the same dimensions as A, with\nA(q)\nij =\nq\naq\nij.\nIf A(q) and\n\u0000\nA(2\u2212q)\u0001\u2020\nare both stored in quantum accessible data structures, then there exist unitaries",
    "A(q)\nij =\nq\naq\nij.\nIf A(q) and\n\u0000\nA(2\u2212q)\u0001\u2020\nare both stored in quantum accessible data structures, then there exist unitaries\nUR and UL that can be implemented in time O(poly(n log(1/\u03b5))) such that U\u2020\nRUL is a (\u00b5q(A), n+ 2, \u03b5)-\nblock-encoding of A. On the other hand, if A is stored in a quantum accessible data structure, then there\nexist unitaries UR and UL that can be implemented in time O(poly(n log(1/\u03b5))) such that U\u2020\nRUL is an\n(\u2225A\u2225F , n+ 2, \u03b5)-block-encoding of A.\nAnother method that may be useful, especially for the transformer architecture is for the Gram matrix\nwhose entries are given by the inner products.\nLemma S3 (Block-encoding of Gram matrices by state preparation unitaries) . Let UL and UR be state\npreparation unitaries acting on a+n qubits preparing the vectors {|\u03c8i\u27e9 : i \u2208 [2n] \u2212 1} and {|\u03d5j\u27e9 : j \u2208 [2n] \u2212 1}\nsuch that\nUL : |0\u27e9|i\u27e9 \u2192 |\u03c8i\u27e9 (A.1)\nUR : |0\u27e9|j\u27e9 \u2192 |\u03d5j\u27e9, (A.2)\nThen U = U\u2020\nLURis an (1, a,0)-block-encoding of the Gram matrix A such that Aij = \u27e8\u03c8i|\u03d5j\u27e9.",
    "such that\nUL : |0\u27e9|i\u27e9 \u2192 |\u03c8i\u27e9 (A.1)\nUR : |0\u27e9|j\u27e9 \u2192 |\u03d5j\u27e9, (A.2)\nThen U = U\u2020\nLURis an (1, a,0)-block-encoding of the Gram matrix A such that Aij = \u27e8\u03c8i|\u03d5j\u27e9.\n25Appendix B: Robust nonlinear amplitude transformation\nTheorem S13 (Robust amplitude encoding). Given an (\u03b1, a, \u03f5)-state-encoding U\u03c8 of an n-qubit state |\u03c8\u27e9 =PN\nj=1 \u03c8j|j\u27e9, where {\u03c8j} are real and \u2225\u03c8\u22252 = 1, one can construct an (\u03b1, 2a+n+2, \u03f5)-encoding of the diagonal\nmatrix A = diag(\u03c81, . . . , \u03c8N ) with O(n) circuit depth and O(1) queries to controlled- U and controlled-U\u2020.\nOne can also construct an (\u03b12, 3a + 2n + 2, 3\u03f5)-encoding of diagonal matrix Aabs = diag(\u03c82\n1, . . . , \u03c82\nN ).\nProof. The construction is the same as Ref. [30, 34] and our focus is on the error analysis. The ( \u03b1, a, \u03f5)-\nstate-encoding U\u03c8 approximately prepares the state\nU|0\u27e9|0\u27e9 = 1\n\u03b1|0\u27e9|\u03c8\u27e9 +\np\n1 \u2212 \u03b12|1\u27e9|bad\u27e9, (B.1)\nwhere |bad\u27e9 is a quantum state we are not interested. By the diagonal amplitude block-encoding introduced",
    "U|0\u27e9|0\u27e9 = 1\n\u03b1|0\u27e9|\u03c8\u27e9 +\np\n1 \u2212 \u03b12|1\u27e9|bad\u27e9, (B.1)\nwhere |bad\u27e9 is a quantum state we are not interested. By the diagonal amplitude block-encoding introduced\nin Ref. [30, 34], one can approximately construct a block-encoding of A = diag( \u03c81, . . . , \u03c8N ). By direct\ncomputation, one can see it is an (\u03b1, 2a+n+2, \u03f5)-encoding, where \u03b1 is directly from the state-encoding, and\nthe error can be obtained from the L\u221e-norm. Let the exact block-encoded diagonal matrix be A\u2032. Note that\n\u2225A\u2212A\u2032\u2225 = maxj |\u03c8j \u2212\u03c8\u2032\nj| = \u2225|\u03c8\u27e9\u2212|\u03c8\u2032\u27e9\u2225\u221e \u2264 \u03f5. Block-encoding of Aabs can be constructed following Theorem\n2 in Ref. [48] and Ref. [30, 34]. The error analysis follows max j |\u03c82\nj \u2212 \u03c8\u2032\nj|2 \u2264 maxj |\u03c82\nj \u2212 (\u03c8j + \u03f5)2| \u22643\u03f5.\nQuery complexity analysis follows the previous results.\nAppendix C: Matrix maximum entry norm\nThe standard block encoding assumption directly tells us about the matrix norm of the block-encoded\nmatrix, i.e., \u2225A\u2225 \u2264\u03b1. With the following lemma, the condition also tells us that max i,j |Aij| \u2264\u03b1, i.e., the",
    "matrix, i.e., \u2225A\u2225 \u2264\u03b1. With the following lemma, the condition also tells us that max i,j |Aij| \u2264\u03b1, i.e., the\nabsolute value of each element is also bounded by \u03b1.\nLemma S4. For a complex matrix A \u2208 Cn\u00d7m, maxi,j |Aij| \u2264 \u2225A\u2225.\nProof. Let \u03c3max(A) be the largest singular value of A. By definition, we have \u2225A\u2225 = \u03c3max(A). Consider the\nsingular value decomposition A = U\u03a3V \u2020, where U and V are unitaries and \u03a3 is a diagonal matrix. Let {fi}i\nand {gj}j be the basis of Cn and Cm respectively. Since U and V are unitaries, we have\n\u2225U\u2020fi\u2225 = \u2225V \u2020gj\u2225 = 1. (C.1)\nWrite v = V \u2020gj. We have\n\u2225Aij\u2225 = |\u27e8fi, Agj\u27e9| = |\u27e8fi, U\u03a3V \u2020gj\u27e9| = |\u27e8U\u2020fi, \u03a3V \u2020gj\u27e9| \u2264 \u2225U\u2020fi\u2225\u2225\u03a3V \u2020gj\u2225\n= \u2225 \u03a3V \u2020gj\u2225 =\n X\nk\n(\u03a3v)2\nk\n!1\n2\n=\n\uf8eb\n\uf8ec\uf8ed\nX\nk\n\uf8eb\n\uf8edX\nj\n\u03a3kjvj\n\uf8f6\n\uf8f8\n2\uf8f6\n\uf8f7\uf8f8\n1\n2\n=\n X\nk\n\u03a32\nkkv2\nk\n!1\n2\n\u2264\n X\nk\n\u03c32\nmaxv2\nk\n!1\n2\n= \u03c3max\u2225v\u22252 = \u2225A\u2225. (C.2)\nAppendix D: Normalized error bound\nHere, we show some results that are useful when considering the conversion from matrix block encoding\nto state preparation encoding.",
    "k\n!1\n2\n= \u03c3max\u2225v\u22252 = \u2225A\u2225. (C.2)\nAppendix D: Normalized error bound\nHere, we show some results that are useful when considering the conversion from matrix block encoding\nto state preparation encoding.\n26Lemma S5. For two d-dimensional vectors \u03c8 = (\u03c81, . . . , \u03c8d) and \u03c8\u2032 = (\u03c8\u2032\n1, . . . , \u03c8\u2032\nd), if |\u03c8j \u2212 \u03c8\u2032\nj| \u2264\u03f5 for\neach j \u2208 [d], we have\n\r\r\r 1\nC \u03c8 \u2212 1\nC\u2032 \u03c8\u2032\n\r\r\r\n2\n\u2264 2\n\u221a\nd\u03f5\nC +\ns\n2\u03f5\n\u221a\nd\nC , (D.1)\nwhere C = \u2225\u03c8\u22252 and C\u2032 = \u2225\u03c8\u2032\u22252.\nProof. By direct computation, we have\n\r\r\r 1\nC\nX\nj\u2208S\n\u03c8j|j\u27e9 \u22121\nC\u2032\nX\nj\u2208S\n\u03c8\u2032\nj|j\u27e9\n\r\r\r\n2\n= 1\nCC\u2032\n\r\r\rC\u2032 X\nj\u2208S\n\u03c8j|j\u27e9 \u2212C\nX\nj\u2208S\n\u03c8\u2032\nj|j\u27e9\n\r\r\r\n2\n= 1\nCC\u2032\n\r\r\rC\u2032\n\u0010X\nj\u2208S\n\u03c8j|j\u27e9 \u2212\nX\nj\u2208S\n\u03c8\u2032\nj|j\u27e9\n\u0011\n+ (C\u2032 \u2212 C)\nX\nj\u2208S\n\u03c8\u2032\nj|j\u27e9\n\r\r\r\n2\n\u2264 1\nCC\u2032\n\uf8eb\n\uf8ed\n\r\r\rC\u2032\n\u0010X\nj\u2208S\n\u03c8j|j\u27e9 \u2212\nX\nj\u2208S\n\u03c8\u2032\nj|j\u27e9\n\u0011\r\r\r\n2\n+\n\r\r\r(C\u2032 \u2212 C)\nX\nj\u2208S\n\u03c8\u2032\nj|j\u27e9\n\r\r\r\n2\n\uf8f6\n\uf8f8, (D.2)\nwhere the inequality comes from the triangle inequality. The first term can be easily bounded by\n\u221a\nd\u03f5/C\nsince for each j \u2208 [d], we have |\u03c8j \u2212 \u03c8\u2032\nj| \u2264\u03f5. Note that for nonnegative real numbers a and b, we have\n|a \u2212 b| = |(\u221aa \u2212\n\u221a\nb)(\u221aa +\n\u221a\nb)| = |\u221aa \u2212\n\u221a\nb||\u221aa +\n\u221a",
    "\u221a\nd\u03f5/C\nsince for each j \u2208 [d], we have |\u03c8j \u2212 \u03c8\u2032\nj| \u2264\u03f5. Note that for nonnegative real numbers a and b, we have\n|a \u2212 b| = |(\u221aa \u2212\n\u221a\nb)(\u221aa +\n\u221a\nb)| = |\u221aa \u2212\n\u221a\nb||\u221aa +\n\u221a\nb| \u2265 |\u221aa \u2212\n\u221a\nb|2, hence |\u221aa \u2212\n\u221a\nb| \u2264\np\n|a \u2212 b|. The\nsecond term can be bounded with the following computation:\n1\nC |C \u2212 C\u2032| \u2264\np\n|C2 \u2212 C\u20322|\nC\n\u2264\nq\n|P\nj\u2208S(\u03c82\nj \u2212 \u03c8\u20322\nj )|\nC\n\u2264\nqP\nj\u2208S|(\u03c8j \u2212 \u03c8\u2032\nj)(\u03c8j + \u03c8\u2032\nj)|\nC\n\u2264\nq\n\u03f5 P\nj\u2208S|\u03c8j + \u03c8\u2032\nj|\nC\n\u2264\nq\n\u03f5 P\nj\u2208S(2|\u03c8j| + \u03f5)\nC\n\u2264\nq\nd\u03f52 + 2\u03f5 P\nj\u2208S|\u03c8j|\nC\n\u2264\n\u221a\nd\u03f5\nC +\nq\n2\u03f5 P\nj\u2208S|\u03c8j|\nC\n\u2264\n\u221a\nd\u03f5\nC +\ns\n2\u03f5\n\u221a\nd\nC , (D.3)\nwhere the last inequality is from the inequality between L1 and L2 norm. Combining these two terms\ntogether, we achieve our final result.\nLemma S6. For two d-dimensional vectors \u03c8 = (\u03c81, . . . , \u03c8d) and \u03c8\u2032 = (\u03c8\u2032\n1, . . . , \u03c8\u2032\nd), if |\u03c8j \u2212 \u03c8\u2032\nj| \u2264\u03f5 for\neach j \u2208 [d], we have\n\r\r\r 1\nC \u03c8 \u2212 1\nC\u2032 \u03c8\u2032\n\r\r\r\n\u221e\n\u2264 (\n\u221a\nd + 1)\u03f5\nC +\ns\n2\u03f5\n\u221a\nd\nC , (D.4)\n27where C = \u2225\u03c8\u22252 and C\u2032 = \u2225\u03c8\u2032\u22252.\nProof. Note that the L\u221e distance can be written as\n\r\r\r 1\nC \u03c8 \u2212 1\nC\u2032 \u03c8\u2032\n\r\r\r\n\u221e\n= max\nj\u2208[d]\n\f\f\f\u03c8j\nC \u2212 \u03c8\u2032\nj\nC\u2032\n\f\f\f. (D.5)",
    "C\u2032 \u03c8\u2032\n\r\r\r\n\u221e\n\u2264 (\n\u221a\nd + 1)\u03f5\nC +\ns\n2\u03f5\n\u221a\nd\nC , (D.4)\n27where C = \u2225\u03c8\u22252 and C\u2032 = \u2225\u03c8\u2032\u22252.\nProof. Note that the L\u221e distance can be written as\n\r\r\r 1\nC \u03c8 \u2212 1\nC\u2032 \u03c8\u2032\n\r\r\r\n\u221e\n= max\nj\u2208[d]\n\f\f\f\u03c8j\nC \u2212 \u03c8\u2032\nj\nC\u2032\n\f\f\f. (D.5)\nWe consider each element individually as\n\f\f\f\u03c8j\nC \u2212 \u03c8\u2032\nj\nC\u2032\n\f\f\f = 1\nCC\u2032 |C\u2032\u03c8j \u2212 C\u03c8\u2032\nj|. (D.6)\nHaving maxj\u2208[d]|\u03c8j \u2212 \u03c8\u2032\nj| \u2264\u03f5, we can write \u03c8j = \u03c8\u2032\nj + \u2206j where |\u2206j| \u2264\u03f5. Substituting \u03c8j in |C\u2032\u03c8j \u2212 C\u03c8\u2032\nj|\nwe have\n|C\u2032\u03c8j \u2212 C\u03c8\u2032\nj| = |C\u2032\u03c8\u2032\nj + C\u2032\u2206j \u2212 C\u03c8\u2032\nj| (D.7)\n= |(C\u2032 \u2212 C)\u03c8\u2032\nj + C\u2032\u2206j| (D.8)\n\u2264 |(C\u2032 \u2212 C)\u03c8\u2032\nj| + C\u2032\u03f5. (D.9)\nThen we can write\nmax\nj\u2208[d]\n\f\f\f\u03c8j\nC \u2212 \u03c8\u2032\nj\nC\u2032\n\f\f\f = max\nj\u2208[d]\n1\nCC\u2032 |C\u2032\u03c8j \u2212 C\u03c8\u2032\nj| (D.10)\n\u2264 C\u2032\u03f5 + maxj\u2208[d]|(C\u2032 \u2212 C)\u03c8\u2032\nj|\nCC\u2032 (D.11)\n\u2264 \u03f5\nC + |C\u2032 \u2212 C|C\u2032\nCC\u2032 (D.12)\n= \u03f5\nC + |C\u2032 \u2212 C|\nC (D.13)\n\u2264 (\n\u221a\nd + 1)\u03f5\nC +\ns\n2\u03f5\n\u221a\nd\nC . (D.14)\nThe bound of |C\u2032\u2212C|\nC directly follows from the proof of Lemma S5.\nLemma S7. For two d-dimensional vectors \u03c8 = (\u03c81, . . . , \u03c8d) and \u03c8\u2032 = (\u03c8\u2032\n1, . . . , \u03c8\u2032\nd), if |\u03c8j \u2212 \u03c8\u2032\nj| \u2264\u03f5 and\n\u03c8j, \u03c8\u2032\nj \u2264 \u0393 \u2208 O(1) for each j \u2208 [d], we have\n\r\r\r 1\nC \u03c8 \u2212 1\nC\u2032 \u03c8\u2032\n\r\r\r\n\u221e\n\u2264 \u03f5",
    "Lemma S7. For two d-dimensional vectors \u03c8 = (\u03c81, . . . , \u03c8d) and \u03c8\u2032 = (\u03c8\u2032\n1, . . . , \u03c8\u2032\nd), if |\u03c8j \u2212 \u03c8\u2032\nj| \u2264\u03f5 and\n\u03c8j, \u03c8\u2032\nj \u2264 \u0393 \u2208 O(1) for each j \u2208 [d], we have\n\r\r\r 1\nC \u03c8 \u2212 1\nC\u2032 \u03c8\u2032\n\r\r\r\n\u221e\n\u2264 \u03f5\nC + \u0393\n\u221a\nd\u03f5\nCC\u2032 + \u0393\nC\u2032\ns\n2\u03f5\n\u221a\nd\nC , (D.15)\nwhere C = \u2225\u03c8\u22252 and C\u2032 = \u2225\u03c8\u2032\u22252.\nProof. Note that the L\u221e distance can be written as\n\r\r\r 1\nC \u03c8 \u2212 1\nC\u2032 \u03c8\u2032\n\r\r\r\n\u221e\n= max\nj\u2208[d]\n\f\f\f\u03c8j\nC \u2212 \u03c8\u2032\nj\nC\u2032\n\f\f\f. (D.16)\nWe consider each element individually as\n\f\f\f\u03c8j\nC \u2212 \u03c8\u2032\nj\nC\u2032\n\f\f\f = 1\nCC\u2032 |C\u2032\u03c8j \u2212 C\u03c8\u2032\nj|. (D.17)\nHaving maxj\u2208[d]|\u03c8j \u2212 \u03c8\u2032\nj| \u2264\u03f5, we can write \u03c8j = \u03c8\u2032\nj + \u2206j where |\u2206j| \u2264\u03f5. Substituting \u03c8j in |C\u2032\u03c8j \u2212 C\u03c8\u2032\nj|\nwe have\n|C\u2032\u03c8j \u2212 C\u03c8\u2032\nj| = |C\u2032\u03c8\u2032\nj + C\u2032\u2206j \u2212 C\u03c8\u2032\nj| (D.18)\n28= |(C\u2032 \u2212 C)\u03c8\u2032\nj + C\u2032\u2206j| (D.19)\n\u2264 |(C\u2032 \u2212 C)\u03c8\u2032\nj| + C\u2032\u03f5. (D.20)\nThen we can write\nmax\nj\u2208[d]\n\f\f\f\u03c8j\nC \u2212 \u03c8\u2032\nj\nC\u2032\n\f\f\f = max\nj\u2208[d]\n1\nCC\u2032 |C\u2032\u03c8j \u2212 C\u03c8\u2032\nj| (D.21)\n\u2264 C\u2032\u03f5 + maxj\u2208[d]|(C\u2032 \u2212 C)\u03c8\u2032\nj|\nCC\u2032 (D.22)\n\u2264 \u03f5\nC + \u0393|C\u2032 \u2212 C|\nCC\u2032 (D.23)\n= \u03f5\nC + \u0393\n\u221a\nd\u03f5\nCC\u2032 + \u0393\nC\u2032\ns\n2\u03f5\n\u221a\nd\nC . (D.24)\nAppendix E: Polynomial approximation of exponential function",
    "j| (D.21)\n\u2264 C\u2032\u03f5 + maxj\u2208[d]|(C\u2032 \u2212 C)\u03c8\u2032\nj|\nCC\u2032 (D.22)\n\u2264 \u03f5\nC + \u0393|C\u2032 \u2212 C|\nCC\u2032 (D.23)\n= \u03f5\nC + \u0393\n\u221a\nd\u03f5\nCC\u2032 + \u0393\nC\u2032\ns\n2\u03f5\n\u221a\nd\nC . (D.24)\nAppendix E: Polynomial approximation of exponential function\nHere we describe how to approximate the exponential function efficiently by a polynomial for x \u2208 [\u22121, 1].\nLemma S8. For x \u2208 [\u22121, 1], the function f(x) := ex can be approximated with error bound \u03f5 with an\nO(log(1/\u03f5))-degree polynomial function.\nProof. Consider the Taylor expansion off(x) = P\u221e\nj=0\nxj\nj! . Let fk(x) := Pk\nj=0\nxj\nj! . To achieve |fk(x)\u2212f(x)| \u2264\n\u03f5 for |x| \u22641,\n|fk(x) \u2212 f(x)| =\n\f\f\f\f\f\f\n\u221eX\nj=k+1\nxj\nj!\n\f\f\f\f\f\f\n\u2264 |\n\u221eX\nj=k+1\n1\nj!| =\n\f\f\f\f\f\f\n\u221eX\nj=1\n1\n(j + k)!\n\f\f\f\f\f\f\n(Assume k >2) \u2264 1\nk!\n\f\f\f\f\f\f\n\u221eX\nj=1\n1\n2j\n\f\f\f\f\f\f\n\u2264 1\nk! \u2264 \u03f5.\nIt suffices to set k = O(log(1\n\u03f5 )), which can be seen by the Stirling\u2019s approximation.\nAppendix F: General case of quantum residual connection\nWe first provide the theorem for only quantum residual connection, which might be an additional interest.",
    "Appendix F: General case of quantum residual connection\nWe first provide the theorem for only quantum residual connection, which might be an additional interest.\nProblem 4 (Quantum residual connection) . Let c > 0 and g(x) be a real k-degree polynomial function.\nGiven an (\u03b1, a, \u03f5)-state-encoding U of a quantum state Pd\nj=1 xj|j\u27e9, where {xj} are real and \u2225x\u22252 = 1 ,\nprepare a state-encoding of the state\n1qPd\nj=1(c \u00b7 g(x)j + xj)2\ndX\nj=1\n(c \u00b7 g(x)j + xj)|j\u27e9. (F.1)\nTheorem S14 (Quantum residual connection). Consider the setting of Problem 4. For the polynomial g(x),\nlet gmax := max x\u2208[\u22121,1]|g(\u03b1x)|, one can prepare an\n\u0000\nO\n\u0000\u221a\nN(\u03b1 + 2cgmax)/C\n\u0001\n, a+ n + 4, O\n\u0000\n(cgmax(4\u2113\u221a\u03f5 +\n\u03b4) + \u03b1\u03f5)/C\n\u0001\u0001\n-state-encoding of the state 1\nC\nPN\nk=1(c \u00b7 g(xk) + xk)|k\u27e9, where C2 := PN\nk=1(c \u00b7 g(xk) + xk)2.\nFurther, if g(x)/x is bounded with \u03b7 := maxx\u2208[\u22121,1]|g(\u03b1x)/x|, one can prepare an\n\u0000\nO(\u03b1(1 + 2c\u03b7)/C), a+ n +\n4, O(c\u03b7(4\u2113\u221a\u03f5 + \u03b4)/C)\n\u0001\n-state-encoding instead. The preparation uses O(\u2113) times of Ux and U\u2020\nx.",
    "\u0000\nO(\u03b1(1 + 2c\u03b7)/C), a+ n +\n4, O(c\u03b7(4\u2113\u221a\u03f5 + \u03b4)/C)\n\u0001\n-state-encoding instead. The preparation uses O(\u2113) times of Ux and U\u2020\nx.\n29Proof. We first discuss the general case. Given the state-encoding Ux, by Theorem 6, one can construct\nan (\u03b1, a+ n + 2, \u03f5)-encoding of A = diag(x1, . . . , xN ). Let gmax := maxx\u2208[\u22121,1]|g(\u03b1x)|, then by Theorem 3\nwith function g(x)/(2gmax), one can construct a (2 gmax, a+ n + 4, 2gmax(4\u2113\u221a\u03f5 + \u03b4))-encoding of the matrix\ndiag(g(x1), . . . , g(xN )). Note that the normalization factor 2gmax is to satisfy the requirements of Theorem 3.\nBy using the linear combination of block-encoded matrices as Lemma 1 with state preparation pair\n(P, P), where P : |0\u27e9 \u21921/\u221a\u03b1 + 2cgmax(\u221a\u03b1|0\u27e9 + \u221a2cgmax|1\u27e9), one can construct an ( \u03b1 + 2cgmax, a+ n +\n5, 2cgmax(4\u2113\u221a\u03f5 + \u03b4) +\u03b1\u03f5)-encoding Ug of the matrix diag( c \u00b7 g(x1) +x1, . . . , c\u00b7 g(xN ) +xN ). One can easily\nverify that Ug(I \u2297 Hn) is a state-encoding of the target state 1\nC\nPN\nk=1(c \u00b7 g(xk) + xk)|k\u27e9. We have\nUg(I \u2297 Hn)|0\u27e9|0\u27e9 = 1\u221a",
    "verify that Ug(I \u2297 Hn) is a state-encoding of the target state 1\nC\nPN\nk=1(c \u00b7 g(xk) + xk)|k\u27e9. We have\nUg(I \u2297 Hn)|0\u27e9|0\u27e9 = 1\u221a\nN(\u03b1 + 2cgmax)\n|0\u27e9\nNX\nk=1\n\u03c8k|k\u27e9 + |e\u22a5\u27e9\n= C\u2032\n\u221a\nN(\u03b1 + 2cgmax)\n|0\u27e9 1\nC\u2032\nNX\nk=1\n\u03c8k|k\u27e9 + |e\u22a5\u27e9, (F.2)\nwhere C\u2032 = \u2225\u03c8\u22252, \u2225\u03c8 \u2212 (c \u00b7 g(x) + x)\u2225\u221e \u2264 2cgmax(4\u2113\u221a\u03f5 + \u03b4) + \u03b1\u03f5, and |e\u22a5\u27e9 is a unnormalized orthogonal\nstate. For simplicity, let \u03f5g := 2cgmax(4\u2113\u221a\u03f5 + \u03b4) + \u03b1\u03f5. By Lemma 4, the final error bound is\n\u03f5g\nC + (cgmax + 1)\nC\u2032\n \u221a\nN\u03f5g\nC +\ns\n2\n\u221a\nN\u03f5g\nC\n!\n= O\n\u0000\n(cgmax(4\u2113\u221a\u03f5 + \u03b4) + \u03b1\u03f5)/C\n\u0001\n.\nNow we consider the specific case, i.e., when the polynomial g(x) has no constant term. Note that for a\npolynomial g(x), if g(x)/x is bounded on the interval across x = 0, it cannot have the constant term. Instead\nof implementing function g(x)/(2gmax) with quantum singular value transformation, here we implement\ng\u2032(A)/2\u03b7 instead, where g\u2032(x) := g(\u03b1x)/x and \u03b7 := maxx\u2208[\u22121,1] |g\u2032(x)|. By Lemma 1 with state preparation",
    "g\u2032(A)/2\u03b7 instead, where g\u2032(x) := g(\u03b1x)/x and \u03b7 := maxx\u2208[\u22121,1] |g\u2032(x)|. By Lemma 1 with state preparation\npair (P\u2032, P\u2032), where P\u2032 : |0\u27e9 \u21921/(\u221a1 + 2c\u03b7)(|0\u27e9+\u221a2c\u03b7|1\u27e9) to construct a (1 + 2c\u03b7, a+n+ 4, 2c\u03b7(4\u2113\u221a\u03f5+\u03b4))-\nencoding of diagonal matrix I + c \u00b7 g\u2032(A). Let this block-encoding unitary be Ug\u2032 and \u03f5g\u2032 := 2c\u03b7(4\u2113\u221a\u03f5 + \u03b4).\nWe have Ug\u2032 (I \u2297 Ux) is the\n\u0012\n\u03b1(1+2c\u03b7)\nC\u2032\u2032 , a+ n + 4,\n\u03f5g\u2032\nC + (c\u03b7+1)\nC\u2032\u2032\n\u0012\u221a\nN\u03f5g\u2032\nC +\nq\n2\n\u221a\nN\u03f5g\u2032\nC\n\u0013\u0013\n-state-encoding of the\ntarget state, where C\u2032\u2032 is the L2 norm for the exact prepared state.\nFor the quantum residual connection and layer normalization, in the main paper, we only mention a\nspecific case, i.e., when \u03b3 = 1/\n\u221a\nd and \u03b2 = 0. If we consider the general layer normalization, the quantum\nstate mentioned in Problem 2 should be\n1\nC\ndX\nk=1\nLN\u03b3,\u03b2 (Gsoft\nj , Sj)k|k\u27e9, (F.3)\nwhere C is the normalization factor. Since vector \u03b2 can be implemented on quantum computers via Theo-",
    "state mentioned in Problem 2 should be\n1\nC\ndX\nk=1\nLN\u03b3,\u03b2 (Gsoft\nj , Sj)k|k\u27e9, (F.3)\nwhere C is the normalization factor. Since vector \u03b2 can be implemented on quantum computers via Theo-\nrem 2, and taking sum via the linear combination of unitaries, here we omit \u03b2. Then the representation of\nthe quantum state can be simplified as\n\u03b3\u221a\nd\ndX\nk=1\nLN\u03b3,0(Gsoft\nj , Sj)k|k\u27e9. (F.4)\nNote that compared to the case which we consider in the main paper, there is an additional factor \u221a\u03b3/\n\u221a\nd =:\n\u03b3\u2032, since now the \u21132-norm is \u03b3\u2032. Now we describe how this factor will affect our analysis. If we continue to\nimplement the feedforward network, we need to implement the function GELU( 1\n\u03b3\u2032 \u00b7) instead of GELU(\u00b7). By\nCorollary 2, the degree of the polynomial for approximating the GELU function will increase O( 1\n\u03b3\u2032 ). For the\nsecond residual connection and layer normalization which is after the feedforward network, this factor does",
    "\u03b3\u2032 ). For the\nsecond residual connection and layer normalization which is after the feedforward network, this factor does\n30not affect the scaling for implementing this block, but the output state will become\n\u03b3\u2032\ndX\nk=1\nTransformer(S, j)|k\u27e9. (F.5)\nIf one wants to obtain the information via quantum state tomography using Theorem 12 with final precision\nO(\u03f5), one needs to set \u03b4 = O(\u03f5\u03b3\u2032) in Theorem 12. An specific case is when \u03b3\u2032 = 1/\n\u221a\nd, i.e., \u03b3 = 1. Under\nsuch case, our results in Theorem 11 will have another factor\n\u221a\nd. Note that this does not affect our result\nmuch as N is the dominant factor rather than d.\nAppendix G: Quantum single-layer transformer\nIn this section, we combine the previous theorems to obtain the final main theorem.\nProof of Theorem 11. As shown in Fig. 1, a single-layer transformer contains the self-attention, residual\nconnection and layer normalization, and the feedforward network. In Theorem 8, 9 and 10, we have considered",
    "connection and layer normalization, and the feedforward network. In Theorem 8, 9 and 10, we have considered\neach block in detail. Here, we complete the analysis for the second residual connection after the feedforward\nnetwork.\nAs described in Problem 3 and Theorem 10, we have access to ( \u03b1, a, \u03f5)-state-encoding of |\u03c8\u27e9 and\n(2\u03b1\u03b12\nm, O(a + n + am), O((\n\u221a\nd\u03b12\nm\u2113\u221a\u03b1m\u03f5 + \u03f5m)\n1\n2 ))-encoding of matrix B such that B\u22c61 = ( \u02dc\u03d51, \u00b7\u00b7\u00b7 , \u02dc\u03d5d),\nwhere \u02dc\u03d5 := M2 \u00b7GELU(M1 \u00b7\u03c8). Here, the dimension of vector \u03c8 is d and N2 = d. The target is to construct\na state encoding of\ndX\nk=1\nLN(\u02dc\u03d5k + \u03c8k)|k\u27e9. (G.1)\nThe state encoding can be understood as a block encoding of a matrix whose first column corresponds to\nthe quantum state. By Lemma 1 and taking the self-adjoint, one can construct a (2 \u03b1\u03b12\nm + \u03b1, O(a + n +\nam), O((\n\u221a\nd\u03b12\nm\u2113\u221a\u03b1m\u03f5 + \u03f5m)\n1\n2 ))-encoding of a matrix whose first row is ( \u03c81 + \u02dc\u03d51, . . . , \u03c8d + \u02dc\u03d5d).\nThe following steps are the same as in Theorem 9. One can construct an ( O((\n\u221a\nd + 1)\u03b1\u03b12",
    "am), O((\n\u221a\nd\u03b12\nm\u2113\u221a\u03b1m\u03f5 + \u03f5m)\n1\n2 ))-encoding of a matrix whose first row is ( \u03c81 + \u02dc\u03d51, . . . , \u03c8d + \u02dc\u03d5d).\nThe following steps are the same as in Theorem 9. One can construct an ( O((\n\u221a\nd + 1)\u03b1\u03b12\nm/\u03c2\u2032, O(a + n +\nam), O((\n\u221a\nd\u03b12\nm\u2113\u221a\u03b1m\u03f5 + \u03f5m)\n1\n2 /\u03c2\u2032))-state-encoding of the state\ndX\nk=1\nLN(\u02dc\u03d5k + \u03c8k)|k\u27e9, (G.2)\nwhere \u03c2\u2032 :=\nqPd\nk=1(\u02dc\u03d5k + \u03c8k \u2212 \u00af\u03c8)2 and \u00af\u03c8 := 1\nd\nPd\nk=1(\u02dc\u03d5k + \u03c8k).\nCombining the results with Theorem 8, 9, and 10, one can achieve the final result.\n31"
  ],
  "summary": "This document, \"Quantum linear algebra is all you need for Transformer architectures,\" by Naixu Guo et al., presents a comprehensive framework for implementing a single-layer Transformer architecture on a fault-tolerant quantum computer. The primary goal is to address the immense computational cost of large language models (LLMs) during the inference phase (i.e., generating a prediction from a pre-trained model) by leveraging the power of quantum linear algebra.\n\n### Introduction and Core Problem\n\nThe paper begins by acknowledging the transformative impact of LLMs like GPT-4, which are predominantly based on the Transformer architecture. However, their power comes at a significant computational cost. The authors propose exploring quantum computing as a potential solution, specifically for the inference task of next-token prediction.\n\nThey identify three major challenges in applying quantum computing to LLMs:\n1.  **Big Data Input:** Quantum computers struggle with loading large classical datasets, a problem tied to the practical difficulty of building quantum Random Access Memory (qRAM).\n2.  **Large Parameter Count:** Modern LLMs have billions of parameters, far exceeding the capacity of current or near-term quantum computers.\n3.  **No-Cloning Principle:** Classical computing allows for easy copying and storage of intermediate results, whereas in quantum computing, this is generally impossible and computationally expensive, requiring full state tomography.\n\nTo make the problem tractable, the authors make two crucial simplifications:\n1.  They focus solely on the **inference process**, assuming the Transformer model is already trained.\n2.  They assume the pre-trained weight matrices and the input data are provided as **block encodings**. A block encoding is a standard technique in modern quantum algorithms where a matrix `A` is embedded as a sub-block of a larger, efficiently implementable unitary matrix `U`. This assumption bypasses the data-loading and training challenges, allowing the focus to be on the core computational architecture.\n\n### The Quantum Framework: Block Encodings and QSVT\n\nThe entire quantum algorithm is built upon the framework of **block encodings** and **Quantum Singular Value Transformation (QSVT)**. This modular framework is well-suited for representing and manipulating matrices, making it a natural fit for the linear algebra-heavy operations within a Transformer. The paper's central achievement is to systematically decompose each component of a Transformer layer into a series of quantum operations that can be composed within this framework.\n\n### Main Results: Quantizing the Transformer Components\n\nThe authors develop novel quantum subroutines for each key building block of a standard decoder-only Transformer layer.\n\n**1. Quantum Self-Attention:** This is the most complex component. The classical process involves creating Query (Q), Key (K), and Value (V) matrices, calculating attention scores via `softmax(QK^T / sqrt(d_k))`, and applying these scores to the Value matrix. The quantum implementation proceeds as follows:\n*   **Q, K, V Construction:** The algorithm starts with block encodings of the input sequence and the weight matrices (W_Q, W_K, W_V) and constructs block encodings for the Q, K, and V matrices.\n*   **Attention Score Calculation:** It computes a block encoding for the product `QK^T`.\n*   **Quantum Softmax:** A key technical contribution of the paper is a new subroutine for applying **element-wise functions** to a block-encoded matrix. They use this to implement the softmax function, which involves an exponential function and row-wise normalization. This is a non-trivial task in a quantum context and represents a significant algorithmic development.\n*   **Final Output:** The resulting attention score matrix is then multiplied with the block-encoded Value matrix (V) to produce the final output of the self-attention block, also as a block encoding.\n\n**2. Quantum Residual Connection and Layer Normalization:**\n*   **Residual Connection:** This operation involves adding the input of a block to its output (e.g., `X + SelfAttention(X)`). Since adding quantum states or matrices is not straightforward, the authors show how to construct a block encoding for the sum of two matrices that are themselves given as block encodings.\n*   **Layer Normalization:** This step normalizes the output of the residual connection by subtracting the mean and dividing by the standard deviation. The paper provides a quantum subroutine to perform this normalization on the vector represented by the block-encoded matrix, preparing a block encoding of the normalized result.\n\n**3. Quantum Feed-Forward Network (FFN):** The FFN in a Transformer is typically a two-layer neural network with a non-linear activation function (like ReLU or GeLU). The authors demonstrate how to implement this using block encodings for the FFN's weight matrices and applying the activation function using polynomial approximations, a standard technique compatible with the QSVT framework.\n\n### The Complete Single-Layer Quantum Transformer\n\nBy composing these quantum subroutines in sequence (Self-Attention \u2192 Residual/LayerNorm \u2192 FFN \u2192 Residual/LayerNorm), the paper constructs a single quantum circuit that represents the entire operation of one Transformer layer. The final output is a block encoding of the full single-layer transformation.\n\nThis final block encoding can be used to prepare a quantum state where the amplitudes of the basis states are proportional to the output logits for the next token. This is known as **amplitude encoding**. To predict the next token, one simply measures this quantum state in the computational basis. The probability of measuring a particular outcome (e.g., the index of a word in the vocabulary) corresponds directly to the probability assigned to that token by the Transformer model.\n\n### Numerical Studies and Potential for Quantum Advantage\n\nTo ground their theoretical work, the authors conducted numerical experiments on several open-source LLMs (e.g., GPT-2, LLaMA). They analyzed the properties of the weight matrices (such as their norms) that directly influence the runtime and complexity of the proposed quantum algorithm. This analysis helps verify that the assumptions made in the quantum model are plausible for real-world architectures and provides insight into the parameter regimes where a quantum advantage might be possible.\n\nThe potential for a quantum advantage is complex and not a simple quadratic speedup. The algorithm's runtime depends on several factors, including the embedding dimension `d`, the input sequence length `N`, various matrix norms, and the desired precision `\u03b5`. A quantum advantage is contingent on how these parameters scale in practical LLM applications.\n\n### Conclusion and Future Directions\n\nThe paper successfully provides a detailed, end-to-end quantum blueprint for a single-layer Transformer, translating its architecture into the language of modern, fault-tolerant quantum algorithms. The main contribution is the modular construction of quantum subroutines for all essential components, especially the novel method for applying the softmax function.\n\nHowever, the authors acknowledge significant remaining challenges. The primary hurdle is the strong assumption that pre-trained weights are available as efficient block encodings. Extending the algorithm to deep, multi-layer Transformers is another major step, as errors would accumulate across layers. Despite these limitations, this work establishes a crucial theoretical foundation for applying advanced quantum algorithms to the core architecture of modern AI, paving the way for future research into quantum advantages for large-language models.",
  "created_at": "2025-08-31T23:06:38.295677"
}